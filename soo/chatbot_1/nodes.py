import re
import json
from typing import Dict, Any, List

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage

from utils import find_and_rank_products
from langchain_community.tools.tavily_search import TavilySearchResults

# ===== ëª¨ë¸ & ì›¹ê²€ìƒ‰ =====
llm = ChatOpenAI(model_name="gpt-4o", temperature=0)
search = TavilySearchResults(k=3)

# ===== ë™ì˜ì–´/ì •ê·œí™” =====
SKIN_TYPES = {"ì§€ì„±", "ê±´ì„±", "ë³µí•©ì„±", "ë¯¼ê°ì„±", "ì•„í† í”¼ì„±"}
CONCERNS = {"ë³´ìŠµ", "ì§„ì •", "ë¯¸ë°±", "ì£¼ë¦„/íƒ„ë ¥", "ëª¨ê³µì¼€ì–´", "í”¼ì§€ì¡°ì ˆ", "ì£¼ë¦„", "íƒ„ë ¥"}
CONCERN_SYNONYMS = {
    # ë³´ìŠµ ê³„ì—´
    "ë³´ìŠµê°": "ë³´ìŠµ", "ìˆ˜ë¶„": "ë³´ìŠµ", "ìˆ˜ë¶„ê°": "ë³´ìŠµ", "ìœ ìˆ˜ë¶„": "ë³´ìŠµ",
    # ì§„ì • ê³„ì—´
    "ì§„ì •": "ì§„ì •", "ì¿¨ë§": "ì§„ì •", "ë¶‰ìŒì¦": "ì§„ì •", "í™ì¡°": "ì§„ì •",
    # ë¯¸ë°±/í†¤ì—…
    "ë¯¸ë°±": "ë¯¸ë°±", "í†¤ì—…": "ë¯¸ë°±", "ì¡í‹°": "ë¯¸ë°±",
    # ì£¼ë¦„/íƒ„ë ¥
    "ì£¼ë¦„": "ì£¼ë¦„/íƒ„ë ¥", "íƒ„ë ¥": "ì£¼ë¦„/íƒ„ë ¥", "íƒ„ë ¥ê°": "ì£¼ë¦„/íƒ„ë ¥", "ë¦¬í”„íŒ…": "ì£¼ë¦„/íƒ„ë ¥",
    # ëª¨ê³µ/í”¼ì§€
    "ëª¨ê³µ": "ëª¨ê³µì¼€ì–´", "ëª¨ê³µê´€ë¦¬": "ëª¨ê³µì¼€ì–´", "ëª¨ê³µì¼€ì–´": "ëª¨ê³µì¼€ì–´", "ë¸”ë™í—¤ë“œ": "ëª¨ê³µì¼€ì–´", "ëª¨ê³µ": "ëª¨ê³µ",
    "í”¼ì§€": "í”¼ì§€ì¡°ì ˆ", "ìœ ë¶„": "í”¼ì§€ì¡°ì ˆ", "ë²ˆë“¤ê±°ë¦¼": "í”¼ì§€ì¡°ì ˆ", "ì—¬ë“œë¦„": "í”¼ì§€ì¡°ì ˆ", "íŠ¸ëŸ¬ë¸”": "í”¼ì§€ì¡°ì ˆ", "í”¼ì§€": "í”¼ì§€"
}

def _normalize_concern_label(c: str) -> str:
    c = (c or "").strip()
    if c in CONCERN_SYNONYMS:
        return CONCERN_SYNONYMS[c]
    if ("ë³´ìŠµ" in c) or ("ìˆ˜ë¶„" in c):
        return "ë³´ìŠµ"
    if "ëª¨ê³µ" in c:
        return "ëª¨ê³µì¼€ì–´"
    if ("í”¼ì§€" in c) or ("ë²ˆë“¤" in c) or ("ìœ ë¶„" in c) or ("ì—¬ë“œë¦„" in c) or ("íŠ¸ëŸ¬ë¸”" in c):
        return "í”¼ì§€ì¡°ì ˆ"
    if ("ì§„ì •" in c) or ("ë¶‰" in c) or ("í™ì¡°" in c):
        return "ì§„ì •"
    if ("ë¯¸ë°±" in c) or ("í†¤ì—…" in c) or ("ì¡í‹°" in c):
        return "ë¯¸ë°±"
    if ("ì£¼ë¦„" in c) or ("íƒ„ë ¥" in c) or ("ë¦¬í”„íŒ…" in c):
        return "ì£¼ë¦„/íƒ„ë ¥"
    return c or "ì•Œ ìˆ˜ ì—†ìŒ"


CATEGORY_SYNONYMS = {
    # ìŠ¤í‚¨/í† ë„ˆ
    "í† ë„ˆ": "ìŠ¤í‚¨/í† ë„ˆ",
    "ìŠ¤í‚¨": "ìŠ¤í‚¨/í† ë„ˆ",
    "ìŠ¤í‚¨/í† ë„ˆ": "ìŠ¤í‚¨/í† ë„ˆ",

    # ë¡œì…˜/ì—ë©€ì „(ì—ë©€ì ¼ í‘œê¸°ë„ í¡ìˆ˜)
    "ë¡œì…˜": "ë¡œì…˜/ì—ë©€ì „",
    "ì—ë©€ì „": "ë¡œì…˜/ì—ë©€ì „",
    "ì—ë©€ì ¼": "ë¡œì…˜/ì—ë©€ì „",
    "ë¡œì…˜/ì—ë©€ì „": "ë¡œì…˜/ì—ë©€ì „",
    "ë¡œì…˜/ì—ë©€ì ¼": "ë¡œì…˜/ì—ë©€ì „",

    # ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼
    "ì„¸ëŸ¼": "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼",
    "ì•°í”Œ": "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼",
    "ì—ì„¼ìŠ¤": "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼",
    "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼": "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼",

    # í¬ë¦¼ & ë°¤
    "í¬ë¦¼": "í¬ë¦¼",
    "ë°¤": "ë°¤/ë©€í‹°ë°¤",
    "ë©€í‹°ë°¤": "ë°¤/ë©€í‹°ë°¤",
    "ë°¤/ë©€í‹°ë°¤": "ë°¤/ë©€í‹°ë°¤",

    # í´ë Œì§•
    "í´ë Œì§•í¼": "í´ë Œì§• í¼",
    "í´ë Œì§•": "í´ë Œì§• í¼",
    "í´ë Œì§• í¼": "í´ë Œì§• í¼",

    # ë§ˆìŠ¤í¬
    "ì‹œíŠ¸ë§ˆìŠ¤í¬": "ì‹œíŠ¸ë§ˆìŠ¤í¬",
    "ë§ˆìŠ¤í¬íŒ©": "ì‹œíŠ¸ë§ˆìŠ¤í¬",
    "íŒ©": "ì‹œíŠ¸ë§ˆìŠ¤í¬",

    # ì„ ì¼€ì–´
    "ì„ í¬ë¦¼": "ì„ í¬ë¦¼",
    "ì„ ë¡œì…˜": "ì„ í¬ë¦¼",
    "ìì™¸ì„ ì°¨ë‹¨ì œ": "ì„ í¬ë¦¼",
    "ìì°¨": "ì„ í¬ë¦¼",
}

ALL_CATEGORIES = ["ìŠ¤í‚¨/í† ë„ˆ","ë¡œì…˜/ì—ë©€ì „","ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼","í¬ë¦¼","ë°¤/ë©€í‹°ë°¤","í´ë Œì§• í¼","ì‹œíŠ¸ë§ˆìŠ¤í¬","ì„ í¬ë¦¼"]

# ì•ˆì „ ì„±ë¶„ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸(ê²¹ì¹¨ ë°©ì§€ìš©)
SAFE_BENIGN_INGS = [
    "ê¸€ë¦¬ì„¸ë¦°","íˆì•Œë£¨ë¡ ì‚°","ì†Œë“í•˜ì´ì•Œë£¨ë¡œë„¤ì´íŠ¸","í•˜ì´ì•Œë£¨ë¡œë‹‰ì• ì”¨ë“œ",
    "ì„¸ë¼ë§ˆì´ë“œ","ì„¸ë¼ë§ˆì´ë“œì—”í”¼","íŒí…Œë†€","ìŠ¤ì¿ ì•Œë€","ë² íƒ€ì¸","ì•Œë€í† ì¸",
    "í† ì½”í˜ë¡¤","ì”íƒ„ê²€","í”„ë¡œíŒë‹¤ì´ì˜¬","ë¶€í‹¸ë Œê¸€ë¼ì´ì½œ","íœí‹¸ë Œê¸€ë¼ì´ì½œ",
    "í•˜ì´ë“œë¡ì‹œì•„ì„¸í† í˜ë…¼","ë‹¤ì´ì†Œë“ì´ë””í‹°ì—ì´"
]

# í•œêµ­ì–´ ì¡°ì‚¬/ë¶€í˜¸ ì œê±°
JOSA_SUFFIXES = ("ì€","ëŠ”","ì´","ê°€","ì„","ë¥¼","ì—","ì˜","ë¡œ","ìœ¼ë¡œ","ê³¼","ì™€","ë‘","í•˜ê³ ","ì—ì„œ","ë¶€í„°","ê¹Œì§€","ë„","ìš”")

def _strip_trailing_josa_punct(t: str) -> str:
    if not isinstance(t, str):
        return t
    t = re.sub(r"[!?.,~â€¦Â·]+$", "", t)
    changed = True
    while changed and len(t) > 1:
        changed = False
        for suf in JOSA_SUFFIXES:
            if t.endswith(suf) and len(t) > len(suf):
                t = t[:-len(suf)]
                changed = True
                break
    return t

def _coerce_to_text(x) -> str:
    if isinstance(x, str):
        return x
    if isinstance(x, list):
        parts = []
        for item in x:
            if isinstance(item, dict) and item.get("type") == "text":
                parts.append(item.get("text", ""))
            elif isinstance(item, str):
                parts.append(item)
        return " ".join(p for p in parts if p).strip()
    return str(x)

def _normalize_tokens(text: str) -> List[str]:
    if not isinstance(text, str):
        text = _coerce_to_text(text)
    cleaned = re.sub(r"[\"'`]+", "", text)
    raw = re.split(r"[,/]\s*|\s+", cleaned)
    tokens = []
    for t in raw:
        t = t.strip()
        if not t:
            continue
        t = _strip_trailing_josa_punct(t)
        if t:
            tokens.append(t)
    return tokens

def _messages_to_text(messages, limit=30) -> str:
    out = []
    for m in messages[-limit:]:
        role = "ì‚¬ìš©ì" if isinstance(m, HumanMessage) else "ë„ìš°ë¯¸"
        out.append(f"{role}: {_coerce_to_text(m.content)}")
    return "\n".join(out)

# ===== íŒŒì‹± =====
def _rule_based_parse(s: str) -> Dict[str, Any]:
    tokens = _normalize_tokens(s)
    skin = None
    concerns: List[str] = []
    category = None
    for t in tokens:
        if t in SKIN_TYPES:
            skin = t; continue

        if t in CONCERNS:
            concerns.append("ì£¼ë¦„/íƒ„ë ¥" if t in {"ì£¼ë¦„", "íƒ„ë ¥"} else t); continue
        if t in CONCERN_SYNONYMS:
            concerns.append(CONCERN_SYNONYMS[t]); continue

        if ("ë³´ìŠµ" in t) or ("ìˆ˜ë¶„" in t):
            concerns.append("ë³´ìŠµ"); continue
        if ("ëª¨ê³µ" in t):
            concerns.append("ëª¨ê³µì¼€ì–´"); continue
        if ("í”¼ì§€" in t) or ("ë²ˆë“¤" in t) or ("ìœ ë¶„" in t) or ("ì—¬ë“œë¦„" in t) or ("íŠ¸ëŸ¬ë¸”" in t):
            concerns.append("í”¼ì§€ì¡°ì ˆ"); continue
        if ("ì§„ì •" in t) or ("ë¶‰" in t) or ("í™ì¡°" in t):
            concerns.append("ì§„ì •"); continue
        if ("ë¯¸ë°±" in t) or ("í†¤ì—…" in t) or ("ì¡í‹°" in t):
            concerns.append("ë¯¸ë°±"); continue
        if ("ì£¼ë¦„" in t) or ("íƒ„ë ¥" in t) or ("ë¦¬í”„íŒ…" in t):
            concerns.append("ì£¼ë¦„/íƒ„ë ¥"); continue

        if t in CATEGORY_SYNONYMS:
            category = CATEGORY_SYNONYMS[t]; continue
        if t.endswith("í† ë„ˆ"):
            category = "ìŠ¤í‚¨/í† ë„ˆ"
        elif t in {"ì„¸ëŸ¼", "ì•°í”Œ", "ì—ì„¼ìŠ¤"}:
            category = "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼"

    concerns = list(dict.fromkeys(concerns))
    return {
        "skin_type": skin or "ì•Œ ìˆ˜ ì—†ìŒ",
        "concerns": concerns or ["ì•Œ ìˆ˜ ì—†ìŒ"],
        "category": category or "ì•Œ ìˆ˜ ì—†ìŒ",
    }

def _llm_json_parse(s: str) -> Dict[str, Any]:
    prompt = f"""
    ì•„ë˜ ë¬¸ì¥ì—ì„œ ì‚¬ìš©ì ì •ë³´ë¥¼ JSONìœ¼ë¡œë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
    - í”¼ë¶€ íƒ€ì…: ë¯¼ê°ì„±, ì§€ì„±, ê±´ì„±, ì•„í† í”¼ì„±, ë³µí•©ì„±
    - í”¼ë¶€ ê³ ë¯¼: ë³´ìŠµ, ì§„ì •, ë¯¸ë°±, ì£¼ë¦„/íƒ„ë ¥, ëª¨ê³µì¼€ì–´, í”¼ì§€ì¡°ì ˆ
    - ì œí’ˆ ì¢…ë¥˜: ìŠ¤í‚¨/í† ë„ˆ, ë¡œì…˜/ì—ë©€ì ¼, ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼, í¬ë¦¼, ë°¤/ë©€í‹°ë°¤, í´ë Œì§• í¼, ì‹œíŠ¸ë§ˆìŠ¤í¬, ì„ í¬ë¦¼/ë¡œì…˜
    ëª» ì°¾ìœ¼ë©´ "ì•Œ ìˆ˜ ì—†ìŒ"ìœ¼ë¡œ ì±„ì›Œì£¼ì„¸ìš”.

    ë°˜ë“œì‹œ ìˆœìˆ˜ JSONë§Œ:
    {{
    "skin_type": "...",
    "concerns": ["..."],
    "category": "..."
    }}

    ì…ë ¥: {s}
    """
    resp = llm.invoke(prompt).content.strip()
    try:
        if "{" in resp and "}" in resp:
            resp = resp[resp.index("{"): resp.rindex("}") + 1]
        data = json.loads(resp)
    except Exception:
        data = {"skin_type": "ì•Œ ìˆ˜ ì—†ìŒ", "concerns": ["ì•Œ ìˆ˜ ì—†ìŒ"], "category": "ì•Œ ìˆ˜ ì—†ìŒ"}

    if isinstance(data.get("concerns"), str):
        data["concerns"] = [data["concerns"]]
    cat = (data.get("category") or "").strip()
    data["category"] = CATEGORY_SYNONYMS.get(cat, cat if cat else "ì•Œ ìˆ˜ ì—†ìŒ")

    concerns = data.get("concerns", [])
    if isinstance(concerns, str):
        concerns = [concerns]
    concerns = [_normalize_concern_label(x) for x in concerns if x]
    data["concerns"] = list(dict.fromkeys([c for c in concerns if c and c != "ì•Œ ìˆ˜ ì—†ìŒ"])) or ["ì•Œ ìˆ˜ ì—†ìŒ"]
    return data

def _extract_category_intent(raw_text: str):
    tx = _coerce_to_text(raw_text or "")
    tokens = _normalize_tokens(tx)
    cats = []
    for t in tokens:
        if t in CATEGORY_SYNONYMS:
            cats.append(CATEGORY_SYNONYMS[t])
        elif t.endswith("í† ë„ˆ"):
            cats.append("ìŠ¤í‚¨/í† ë„ˆ")
        elif t in {"ì„¸ëŸ¼","ì•°í”Œ","ì—ì„¼ìŠ¤"}:
            cats.append("ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼")
    # ì¤‘ë³µ ì œê±°
    uniq = []
    for c in cats:
        if c and c not in uniq:
            uniq.append(c)
    return uniq

# ===== ê²€ìƒ‰ ìœ í‹¸ =====
PREFERRED_SITES = ["hwahae.co.kr"]

def search_prefer(query: str):
    for site in PREFERRED_SITES:
        try:
            r = search.run(f"site:{site} {query}")
            if (isinstance(r, str) and r.strip()) or (isinstance(r, list) and len(r) > 0):
                return r
        except Exception:
            pass
    return search.run(query)

# ===== ì›¹ ìš”ì•½ ìœ í‹¸ =====
def fetch_warnings_for_ingredients(ingredients: List[str], efficacy_ings: List[str] = None) -> str:
    if not ingredients:
        return ""
    efficacy_ings = efficacy_ings or []
    web_results = search_prefer(f"{', '.join(ingredients)} í™”ì¥í’ˆ ìœ í•´ì„± ì£¼ì˜ì‚¬í•­")
    prompt = f"""
    ì—­í• : ë‹¹ì‹ ì€ í™”ì¥í’ˆ ì•ˆì „ì„± ìš”ì•½ê°€ì…ë‹ˆë‹¤.
    ìë£Œ:
    ---
    {web_results}
    ---
    ì…ë ¥ ì„±ë¶„: {', '.join(ingredients)}
    íš¨ëŠ¥ ì„±ë¶„(ê²¹ì¹˜ë©´ ì¡°ê±´ë¶€ ì²˜ë¦¬): {', '.join(efficacy_ings)}
    ì¼ë°˜ ë³´ì¡° ì„±ë¶„ì€ ì œì™¸: {', '.join(SAFE_BENIGN_INGS)}
    ê·œì¹™: 3~5ê°œ, í•œ ì¤„ ìš”ì•½, ì¤‘ìš”ë„ ìˆœ.
    ì¶œë ¥:
    - ì„±ë¶„ â€” [ìœ„í—˜|ì¡°ê±´ë¶€] ì‚¬ìœ 
    """
    resp = llm.invoke(prompt)
    txt = (resp.content or "").strip()
    lines = [ln.strip() for ln in txt.splitlines() if ln.strip()]
    return "\n".join(lines[:5])

def fetch_reasons_for_products(products: List[dict], selections: Dict[str, Any], key_ingredients: List[str]) -> List[str]:
    reasons: List[str] = []
    skin = selections.get("skin_type", "ì•Œ ìˆ˜ ì—†ìŒ")
    concerns = ", ".join([c for c in selections.get("concerns", []) if c and c != "ì•Œ ìˆ˜ ì—†ìŒ"]) or "ì•Œ ìˆ˜ ì—†ìŒ"
    category = selections.get("category", "ì•Œ ìˆ˜ ì—†ìŒ")
    import re
    for p in products[:3]:
        brand = (p.get("brand") or "").strip()
        name = (p.get("name") or "").strip()
        matched = ", ".join(sorted(set([m for m in p.get("found_ingredients", []) if m]))) or ", ".join(key_ingredients)
        query = f"{brand} {name} ì„±ë¶„ íš¨ê³¼ ë¦¬ë·° ì¥ë‹¨ì  {category} {skin} {concerns}"
        try:
            web_results = search_prefer(query)
        except Exception as e:
            web_results = f"(ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e})"
        prompt = f"""
        ì—­í• : í™”ì¥í’ˆ ì¶”ì²œ ê·¼ê±° ìš”ì•½ê°€
        ì¡°ê±´: í”¼ë¶€={skin}, ê³ ë¯¼={concerns}, ì¹´í…Œê³ ë¦¬={category}
        ì œí’ˆ: {brand} {name}
        ë§¤ì¹­ ì„±ë¶„: {matched}
        ìë£Œ:
        ---
        {web_results}
        ---
        ê·œì¹™: 35~60ì í•œ ì¤„, ê·¼ê±° ê¸°ë°˜, í—ˆìœ„ ê¸ˆì§€, ìë£Œ ë¶€ì¡± ì‹œ ë§¤ì¹­ ì„±ë¶„ ê¸°ë°˜
        """
        try:
            resp = llm.invoke(prompt)
            reason = (resp.content or "").strip().splitlines()[0]
        except Exception:
            reason = ""
        reason = re.sub(r"^[â€¢\-\*\d\.\)\s]+", "", reason) or "í•µì‹¬ ì„±ë¶„ê³¼ ì €ìê·¹ ì§€í‘œ ê¸°ë°˜ ì¶”ì²œ"
        reasons.append(reason)
    return reasons

# ===== LangGraph ë…¸ë“œ =====
def parse_user_input(state: Dict[str, Any]):
    """
    1) í˜„ì¬ ë¬¸ì¥ íŒŒì‹±
    2) ì´ì „ prefsì™€ ë¨¸ì§€ (í˜„ì¬ ë¬¸ì¥ì— ì—†ëŠ” í•­ëª©ì€ ìœ ì§€)
    3) ë³‘í•© ê²°ê³¼ë¥¼ user_selectionsì™€ prefsì— ë™ê¸°í™”
    """
    last = ""
    for m in reversed(state.get("messages", [])):
        if isinstance(m, HumanMessage):
            last = _coerce_to_text(m.content)
            break

    # í˜„ì¬ ë¬¸ì¥ íŒŒì‹±
    parsed = _rule_based_parse(last)

    # ì¹´í…Œê³ ë¦¬ë§Œ ë§í•œ í›„ì†(ì˜ˆ: "í† ë„ˆë„", "ì„ í¬ë¦¼ì€?")
    if parsed.get("category") == "ì•Œ ìˆ˜ ì—†ìŒ":
        cats = _extract_category_intent(last)
        if cats:
            parsed["category"] = cats[0]

    # ê³¼ê±° prefs ë¶ˆëŸ¬ì™€ì„œ ë³‘í•©
    prev = state.get("prefs") or {"skin_type": "ì•Œ ìˆ˜ ì—†ìŒ", "concerns": ["ì•Œ ìˆ˜ ì—†ìŒ"], "category": "ì•Œ ìˆ˜ ì—†ìŒ"}
    merged = {
        "skin_type": parsed["skin_type"] if parsed["skin_type"] != "ì•Œ ìˆ˜ ì—†ìŒ" else prev.get("skin_type", "ì•Œ ìˆ˜ ì—†ìŒ"),
        "concerns": parsed["concerns"] if parsed["concerns"] != ["ì•Œ ìˆ˜ ì—†ìŒ"] else prev.get("concerns", ["ì•Œ ìˆ˜ ì—†ìŒ"]),
        "category": parsed["category"] if parsed["category"] != "ì•Œ ìˆ˜ ì—†ìŒ" else prev.get("category", "ì•Œ ìˆ˜ ì—†ìŒ"),
    }

    # ì•ˆì „ ë³´ì •
    if not merged.get("skin_type"): merged["skin_type"] = "ì•Œ ìˆ˜ ì—†ìŒ"
    if not merged.get("concerns"): merged["concerns"] = ["ì•Œ ìˆ˜ ì—†ìŒ"]
    if not merged.get("category"): merged["category"] = "ì•Œ ìˆ˜ ì—†ìŒ"

    # ìƒíƒœì— ì €ì¥(ë‹¤ìŒ í„´ì—ì„œ ì‚¬ìš©)
    state["prefs"] = merged
    return {"user_selections": merged, "prefs": merged}

def check_parsing_status(state: Dict[str, Any]):
    s = state["user_selections"]
    has_category = s.get("category") and s["category"] != "ì•Œ ìˆ˜ ì—†ìŒ"
    has_skin = s.get("skin_type") and s["skin_type"] != "ì•Œ ìˆ˜ ì—†ìŒ"
    has_concerns = s.get("concerns") and s["concerns"] != ["ì•Œ ìˆ˜ ì—†ìŒ"]
    # âœ… ì¹´í…Œê³ ë¦¬ + (í”¼ë¶€íƒ€ì… or ê³ ë¯¼) ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì§„í–‰
    return "success" if (has_category and (has_skin or has_concerns)) else "clarification_needed"

def ask_for_clarification(state: Dict[str, Any]):
    s = state["user_selections"]
    need_category = (s.get("category") in (None, "", "ì•Œ ìˆ˜ ì—†ìŒ"))
    need_skin = (s.get("skin_type") in (None, "", "ì•Œ ìˆ˜ ì—†ìŒ"))
    need_concerns = (not s.get("concerns")) or (s.get("concerns") == ["ì•Œ ìˆ˜ ì—†ìŒ"])
    missing = []
    if need_category: missing.append("ì œí’ˆ ì¢…ë¥˜")
    if need_skin and need_concerns: missing.append("í”¼ë¶€ íƒ€ì… ë˜ëŠ” í”¼ë¶€ ê³ ë¯¼")
    hint = "ì˜ˆ: 'ì§€ì„±, ë³´ìŠµ, ë¡œì…˜' ë˜ëŠ” 'ì§€ì„±, ë¡œì…˜' ë˜ëŠ” 'ë³´ìŠµ, ë¡œì…˜'"
    text = f"ë¶€ì¡±í•œ ì •ë³´({', '.join(missing)})ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. {hint}"
    return {"messages": [AIMessage(content=text)]}

def get_ingredients(state: Dict[str, Any]):
    s = state["user_selections"]
    skin_type = s.get("skin_type", "ì•Œ ìˆ˜ ì—†ìŒ")
    concerns = s.get("concerns", ["ì•Œ ìˆ˜ ì—†ìŒ"])
    skin_label = skin_type if skin_type != "ì•Œ ìˆ˜ ì—†ìŒ" else "ì¼ë°˜ì ì¸"

    if concerns == ["ì•Œ ìˆ˜ ì—†ìŒ"]:
        prompt_text = f"""
        ì—­í• : í™”ì¥í’ˆ ì„±ë¶„ íë ˆì´í„°.
        ëª©í‘œ: '{skin_label}' í”¼ë¶€ì— ë³´í¸ì ìœ¼ë¡œ ì•ˆì „í•˜ê³  ìœ íš¨í•œ í•µì‹¬ í™œì„± ì„±ë¶„ 5ê°œë§Œ ì„ ì •.
        ì§€ì¹¨: ìê·¹ ë‚®ê³  ê·¼ê±° ê¸°ë°˜. ë³´ì¡°/ìš©ë§¤/í–¥/ë³´ì¡´ì œ/UVí•„í„° ì œì™¸.
        ì¶œë ¥: ì‰¼í‘œë¡œë§Œ êµ¬ë¶„ëœ í•œ ì¤„
        """
    else:
        prompt_text = f"""
        ì—­í• : í™”ì¥í’ˆ ì„±ë¶„ íë ˆì´í„°.
        ëª©í‘œ: '{skin_label}' í”¼ë¶€ì˜ '{', '.join(concerns)}' ê³ ë¯¼ ê°œì„ ì— ê¸°ì—¬í•˜ëŠ” í•µì‹¬ í™œì„± ì„±ë¶„ 5ê°œë§Œ ì„ ì •.
        ì§€ì¹¨: ê·¼ê±° ê¸°ë°˜ í™œì„± ìœ„ì£¼, ë³´ì¡°/ìš©ë§¤/í–¥/ë³´ì¡´ì œ/UVí•„í„° ì œì™¸.
        ì¶œë ¥: ì‰¼í‘œë¡œë§Œ êµ¬ë¶„ëœ í•œ ì¤„
        """
    resp = llm.invoke(prompt_text)
    key_ingredients_str = resp.content.strip()
    return {"key_ingredients": [ing.strip().lower() for ing in key_ingredients_str.split(",") if ing.strip()]}

from pathlib import Path
DATA_PATH = Path(__file__).parent / "product_data.csv"

def find_products(state: Dict[str, Any]):
    selections = state["user_selections"]
    key_ingredients = state.get("key_ingredients", [])
    # 1ì°¨: í‚¤ì„±ë¶„ ê¸°ë°˜
    top_products = find_and_rank_products(str(DATA_PATH), selections, key_ingredients)
    # í´ë°±: í‚¤ì„±ë¶„ ì—†ì´
    if not top_products:
        top_products = find_and_rank_products(str(DATA_PATH), selections, [])
    return {"top_products": top_products}

def create_recommendation_message(state: Dict[str, Any]):
    import re
    from langchain_core.messages import AIMessage

    top = state.get("top_products", []) or []
    if not top:
        text = "ì¡°ê±´ì— ë§ëŠ” ì œí’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¹´í…Œê³ ë¦¬ì™€ í”¼ë¶€íƒ€ì…/ê³ ë¯¼ ì¤‘ í•˜ë‚˜ ì´ìƒì„ ì•Œë ¤ì£¼ì„¸ìš”."
        return {"messages": [AIMessage(content=text)], "recommendation_message": text}

    lines: List[str] = []
    lines.append("ìš”ì²­í•˜ì‹  ì¡°ê±´ì— ë§ì¶° ì¶”ì²œ ì œí’ˆì„ ì •ë¦¬í–ˆì–´ìš”. ğŸ˜Š")

    s = state.get("user_selections", {}) or {}
    skin = s.get("skin_type", "ì•Œ ìˆ˜ ì—†ìŒ")
    concerns_list = [c for c in s.get("concerns", []) if c and c != "ì•Œ ìˆ˜ ì—†ìŒ"]
    category = s.get("category", "ì•Œ ìˆ˜ ì—†ìŒ")

    cond_bits = []
    if skin != "ì•Œ ìˆ˜ ì—†ìŒ": cond_bits.append(f"ğŸ§‘â€ğŸ¦° í”¼ë¶€: **{skin}**")
    if concerns_list: cond_bits.append(f"ğŸŒ¿ ê³ ë¯¼: **{', '.join(concerns_list)}**")
    if category != "ì•Œ ìˆ˜ ì—†ìŒ": cond_bits.append(f"ğŸ§´ ì œí’ˆ: **{category}**")
    if cond_bits:
        lines.append("**ğŸ¯ ì‚¬ìš©ì ì¡°ê±´**")
        lines.append("   " + "   Â·   ".join(cond_bits))
        lines.append("")

    key_ings = [k.strip() for k in state.get("key_ingredients", []) if k and str(k).strip()]
    if key_ings:
        lines.append(f"**ğŸ§ª íš¨ëŠ¥ ì„±ë¶„(ë¶„ì„ ê¸°ì¤€):** {', '.join(key_ings)}")
        lines.append("")

    web_reasons = fetch_reasons_for_products(top, s, key_ings)

    rank_emojis = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"]
    for i, p in enumerate(top[:3]):
        emoji = rank_emojis[i] if i < len(rank_emojis) else f"{i+1}."
        name = (p.get("name") or "").strip()
        brand = (p.get("brand") or "").strip()
        price = p.get("price", "?")
        volume = p.get("volume", "?")
        link = (p.get("link") or "").strip()
        link_md = f"[ë§í¬]({link})" if link else "-"

        matched_raw = [m.strip() for m in p.get("found_ingredients", []) if m and str(m).strip()]
        matched_unique = ", ".join(sorted(set(matched_raw))) if matched_raw else "ì—†ìŒ (ê¸°ì¤€ ì„±ë¶„ê³¼ ì§ì ‘ ë§¤ì¹­ ì—†ìŒ)"

        lines.append(f"{emoji} {i+1}. {name}")
        lines.append(f"   ğŸ¬ ë¸Œëœë“œ: {brand or '-'}")
        lines.append(f"   ğŸ’° ê°€ê²©: {price}ì›")
        lines.append(f"   ğŸ«™ ìš©ëŸ‰: {volume}")
        lines.append(f"   ğŸ”— {link_md}")
        lines.append(f"   ğŸ§ª íš¨ëŠ¥ ë§¤ì¹­ ì„±ë¶„: {matched_unique}")
        reason = web_reasons[i] if i < len(web_reasons) else "í•µì‹¬ ì„±ë¶„ê³¼ ì €ìê·¹ ì§€í‘œê°€ ì¡°ê±´ì— ë¶€í•©"
        lines.append(f"   âœ… ì¶”ì²œ ì´ìœ : {reason}")
        if i < 2:
            lines.append("")

    # ê°„ë‹¨ ê²½ê³  ìš”ì•½ (ì„ íƒ)
    all_found = [str(ing).strip().lower() for p in top for ing in p.get("found_ingredients", []) if str(ing).strip()]
    unique_found = sorted(set(all_found))
    safe_set = {s.lower() for s in SAFE_BENIGN_INGS}
    warn_candidates = [ing for ing in unique_found if ing not in safe_set]
    warnings_raw = fetch_warnings_for_ingredients(warn_candidates, efficacy_ings=[k.lower() for k in key_ings]) if warn_candidates else ""
    if warnings_raw:
        lines.append("")
        lines.append("âš ï¸ ì£¼ì˜í•´ì•¼ ë  ì„±ë¶„:")
        for ln in [ln.strip() for ln in warnings_raw.splitlines() if ln.strip()][:5]:
            clean = re.sub(r"^[â€¢\-\*\d\.\)\s]+", "", ln)
            lines.append(f"âš ï¸ {clean}")

    final_text = "\n".join(lines)
    return {"messages": [AIMessage(content=final_text)], "recommendation_message": final_text}

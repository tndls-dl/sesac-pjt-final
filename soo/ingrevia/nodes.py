import re
import json
from pathlib import Path
from typing import Dict, Any, List

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage

from utils import find_and_rank_products

# [ADD] ì›¹ ê²€ìƒ‰ (ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©, ì‹¤íŒ¨ ì‹œ ìë™ í´ë°±)
try:
    from langchain_community.tools.tavily_search import TavilySearchResults
    _search = TavilySearchResults(k=3)
except Exception:
    _search = None

SAFE_BENIGN_INGS = [
    "ê¸€ë¦¬ì„¸ë¦°","íˆì•Œë£¨ë¡ ì‚°","ì†Œë“í•˜ì´ì•Œë£¨ë¡œë„¤ì´íŠ¸","í•˜ì´ì•Œë£¨ë¡œë‹‰ì• ì”¨ë“œ",
    "ì„¸ë¼ë§ˆì´ë“œ","ì„¸ë¼ë§ˆì´ë“œì—”í”¼","íŒí…Œë†€","ìŠ¤ì¿ ì•Œë€","ë² íƒ€ì¸","ì•Œë€í† ì¸",
    "í† ì½”í˜ë¡¤","ì”íƒ„ê²€","í”„ë¡œíŒë‹¤ì´ì˜¬","ë¶€í‹¸ë Œê¸€ë¼ì´ì½œ","íœí‹¸ë Œê¸€ë¼ì´ì½œ",
    "í•˜ì´ë“œë¡ì‹œì•„ì„¸í† í˜ë…¼","ë‹¤ì´ì†Œë“ì´ë””í‹°ì—ì´"
]

_PREFERRED_SITES = ["hwahae.co.kr"]
def _search_prefer(query: str):
    if _search is None:
        return ""
    # ìš°ì„  ì„ í˜¸ ì‚¬ì´íŠ¸
    for site in _PREFERRED_SITES:
        try:
            r = _search.run(f"site:{site} {query}")
            if (isinstance(r, str) and r.strip()) or (isinstance(r, list) and len(r) > 0):
                return r
        except Exception:
            pass
    # ì¼ë°˜ ê²€ìƒ‰
    try:
        return _search.run(query)
    except Exception:
        return ""


# =========================
# ëª¨ë¸
# =========================
llm = ChatOpenAI(model="gpt-4o", temperature=0)

# =========================
# ë¼ë²¨/ë™ì˜ì–´
# =========================
SKIN_TYPES = {"ë¯¼ê°ì„±", "ì§€ì„±", "ê±´ì„±", "ë³µí•©ì„±", "ì•„í† í”¼ì„±", "ì¤‘ì„±"}

CONCERNS = {"ë³´ìŠµ", "ì§„ì •", "ë¯¸ë°±", "ì£¼ë¦„/íƒ„ë ¥", "ëª¨ê³µì¼€ì–´", "í”¼ì§€ì¡°ì ˆ", "ì£¼ë¦„", "íƒ„ë ¥"}
CONCERN_SYNONYMS = {
    # ë³´ìŠµ ê³„ì—´
    "ë³´ìŠµê°": "ë³´ìŠµ", "ìˆ˜ë¶„": "ë³´ìŠµ", "ìˆ˜ë¶„ê°": "ë³´ìŠµ", "ìœ ìˆ˜ë¶„": "ë³´ìŠµ",
    # ì§„ì • ê³„ì—´
    "ì§„ì •": "ì§„ì •", "ì¿¨ë§": "ì§„ì •", "ë¶‰ìŒì¦": "ì§„ì •", "í™ì¡°": "ì§„ì •",
    # ë¯¸ë°±/í†¤ì—…
    "ë¯¸ë°±": "ë¯¸ë°±", "í†¤ì—…": "ë¯¸ë°±", "ì¡í‹°": "ë¯¸ë°±",
    # ì£¼ë¦„/íƒ„ë ¥
    "ì£¼ë¦„": "ì£¼ë¦„/íƒ„ë ¥", "íƒ„ë ¥": "ì£¼ë¦„/íƒ„ë ¥", "íƒ„ë ¥ê°": "ì£¼ë¦„/íƒ„ë ¥", "ë¦¬í”„íŒ…": "ì£¼ë¦„/íƒ„ë ¥",
    # ëª¨ê³µ/í”¼ì§€
    "ëª¨ê³µ": "ëª¨ê³µì¼€ì–´", "ëª¨ê³µê´€ë¦¬": "ëª¨ê³µì¼€ì–´", "ëª¨ê³µì¼€ì–´": "ëª¨ê³µì¼€ì–´", "ë¸”ë™í—¤ë“œ": "ëª¨ê³µì¼€ì–´", "ëª¨ê³µ": "ëª¨ê³µ",
    "í”¼ì§€": "í”¼ì§€ì¡°ì ˆ", "ìœ ë¶„": "í”¼ì§€ì¡°ì ˆ", "ë²ˆë“¤ê±°ë¦¼": "í”¼ì§€ì¡°ì ˆ", "ì—¬ë“œë¦„": "í”¼ì§€ì¡°ì ˆ", "íŠ¸ëŸ¬ë¸”": "í”¼ì§€ì¡°ì ˆ", "í”¼ì§€": "í”¼ì§€",
}

CATEGORY_SYNONYMS = {
    # ìŠ¤í‚¨/í† ë„ˆ
    "í† ë„ˆ": "ìŠ¤í‚¨/í† ë„ˆ",
    "ìŠ¤í‚¨": "ìŠ¤í‚¨/í† ë„ˆ",
    "ìŠ¤í‚¨/í† ë„ˆ": "ìŠ¤í‚¨/í† ë„ˆ",

    # ë¡œì…˜/ì—ë©€ì „
    "ë¡œì…˜": "ë¡œì…˜/ì—ë©€ì „",
    "ì—ë©€ì „": "ë¡œì…˜/ì—ë©€ì „",
    "ì—ë©€ì ¼": "ë¡œì…˜/ì—ë©€ì „",
    "ë¡œì…˜/ì—ë©€ì „": "ë¡œì…˜/ì—ë©€ì „",
    "ë¡œì…˜/ì—ë©€ì ¼": "ë¡œì…˜/ì—ë©€ì „",

    # ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼
    "ì„¸ëŸ¼": "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼",
    "ì•°í”Œ": "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼",
    "ì—ì„¼ìŠ¤": "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼",
    "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼": "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼",

    # í¬ë¦¼ & ë°¤
    "í¬ë¦¼": "í¬ë¦¼",
    "ë°¤": "ë°¤/ë©€í‹°ë°¤",
    "ë©€í‹°ë°¤": "ë°¤/ë©€í‹°ë°¤",
    "ë°¤/ë©€í‹°ë°¤": "ë°¤/ë©€í‹°ë°¤",

    # í´ë Œì§•
    "í´ë Œì§•í¼": "í´ë Œì§• í¼",
    "í´ë Œì§•": "í´ë Œì§• í¼",
    "í´ë Œì§• í¼": "í´ë Œì§• í¼",

    # ë§ˆìŠ¤í¬
    "ì‹œíŠ¸ë§ˆìŠ¤í¬": "ì‹œíŠ¸ë§ˆìŠ¤í¬",
    "ë§ˆìŠ¤í¬íŒ©": "ì‹œíŠ¸ë§ˆìŠ¤í¬",
    "íŒ©": "ì‹œíŠ¸ë§ˆìŠ¤í¬",

    # ì„ ì¼€ì–´
    "ì„ í¬ë¦¼": "ì„ í¬ë¦¼",
    "ì„ ë¡œì…˜": "ì„ í¬ë¦¼",
    "ìì™¸ì„ ì°¨ë‹¨ì œ": "ì„ í¬ë¦¼",
    "ìì°¨": "ì„ í¬ë¦¼",
}

ALL_CATEGORIES = ["ìŠ¤í‚¨/í† ë„ˆ","ë¡œì…˜/ì—ë©€ì „","ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼","í¬ë¦¼","ë°¤/ë©€í‹°ë°¤","í´ë Œì§• í¼","ì‹œíŠ¸ë§ˆìŠ¤í¬","ì„ í¬ë¦¼"]

# ì•ˆì „ ì„±ë¶„ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸(ê²¹ì¹¨ ë°©ì§€ìš©)
SAFE_BENIGN_INGS = [
    "ê¸€ë¦¬ì„¸ë¦°","íˆì•Œë£¨ë¡ ì‚°","ì†Œë“í•˜ì´ì•Œë£¨ë¡œë„¤ì´íŠ¸","í•˜ì´ì•Œë£¨ë¡œë‹‰ì• ì”¨ë“œ",
    "ì„¸ë¼ë§ˆì´ë“œ","ì„¸ë¼ë§ˆì´ë“œì—”í”¼","íŒí…Œë†€","ìŠ¤ì¿ ì•Œë€","ë² íƒ€ì¸","ì•Œë€í† ì¸",
    "í† ì½”í˜ë¡¤","ì”íƒ„ê²€","í”„ë¡œíŒë‹¤ì´ì˜¬","ë¶€í‹¸ë Œê¸€ë¼ì´ì½œ","íœí‹¸ë Œê¸€ë¼ì´ì½œ",
    "í•˜ì´ë“œë¡ì‹œì•„ì„¸í† í˜ë…¼","ë‹¤ì´ì†Œë“ì´ë””í‹°ì—ì´"
]

# =========================
# í•œêµ­ì–´ ì¡°ì‚¬/ë¶€í˜¸ ì œê±° & í† í°
# =========================
JOSA_SUFFIXES = ("ì€","ëŠ”","ì´","ê°€","ì„","ë¥¼","ì—","ì˜","ë¡œ","ìœ¼ë¡œ","ê³¼","ì™€","ë‘","í•˜ê³ ","ì—ì„œ","ë¶€í„°","ê¹Œì§€","ë„","ìš”","ì¸ë°")

def _strip_trailing_josa_punct(t: str) -> str:
    if not isinstance(t, str):
        return t
    t = re.sub(r"[!?.~â€¦Â·]+$", "", t)
    changed = True
    while changed and len(t) > 1:
        changed = False
        for suf in JOSA_SUFFIXES:
            if t.endswith(suf) and len(t) > len(suf):
                t = t[:-len(suf)]
                changed = True
                break
    return t

def _coerce_to_text(x) -> str:
    if isinstance(x, str):
        return x
    if isinstance(x, list):
        parts = []
        for item in x:
            if isinstance(item, dict) and item.get("type") == "text":
                parts.append(item.get("text", ""))
            elif isinstance(item, str):
                parts.append(item)
        return " ".join(p for p in parts if p).strip()
    return str(x)

def _normalize_tokens(text: str) -> List[str]:
    if not isinstance(text, str):
        text = _coerce_to_text(text)
    cleaned = re.sub(r"[\"'`]+", "", text)
    raw = re.split(r"[,/]\s*|\s+", cleaned)
    tokens: List[str] = []
    for t in raw:
        t = (t or "").strip()
        if not t:
            continue
        t = _strip_trailing_josa_punct(t)  # â† í•µì‹¬: 'ê±´ì„±ì¸ë°' -> 'ê±´ì„±'
        if t:
            tokens.append(t)
    return tokens

def _extract_category_intent(raw_text: str):
    """
    ë¬¸ì¥ì—ì„œ ì¹´í…Œê³ ë¦¬ ì „í™˜(add/switch) ì˜ë„ë¥¼ ì¶”ì¶œ.
    - 'ë„/ë˜/ì—­ì‹œ'ê°€ í¬í•¨ë˜ë©´ add ëª¨ë“œ(ì´ì „ + ì¶”ê°€)
    - ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œë¥¼ í† í°ì—ì„œ ì¶”ì¶œí•´ í‘œì¤€í™” ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    ë°˜í™˜: (mode, cats)
      mode: "add" ë˜ëŠ” "switch"
      cats: í‘œì¤€í™”ëœ ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    tx = _coerce_to_text(raw_text or "")
    # '~ë„ / ë˜ / ì—­ì‹œ'ê°€ ìˆìœ¼ë©´ add ì˜ë„
    add_mode = bool(re.search(r"(?:^|\s)(ë„|ë˜|ì—­ì‹œ)(?:\s|$)", tx))
    tokens = _normalize_tokens(tx)

    cats = []
    for t in tokens:
        if t in CATEGORY_SYNONYMS:
            cats.append(CATEGORY_SYNONYMS[t])
        elif t.endswith("í† ë„ˆ"):
            cats.append("ìŠ¤í‚¨/í† ë„ˆ")
        elif t in {"ì„¸ëŸ¼", "ì•°í”Œ", "ì—ì„¼ìŠ¤"}:
            cats.append("ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼")

    # ì¤‘ë³µ ì œê±°(ìˆœì„œ ìœ ì§€)
    uniq = []
    for c in cats:
        if c and c not in uniq:
            uniq.append(c)

    mode = "add" if (add_mode and uniq) else "switch"
    return mode, uniq

def _looks_offtopic(text: str) -> bool:
    """ìŠ¤í‚¨ì¼€ì–´ ì˜ë„ ì‹ í˜¸(í”¼ë¶€íƒ€ì…/ê³ ë¯¼/ì¹´í…Œê³ ë¦¬/ì¶”ì²œìš”ì²­)ê°€ ì „í˜€ ì—†ìœ¼ë©´ True."""
    tx = _coerce_to_text(text or "")
    tokens = _normalize_tokens(tx)

    for t in tokens:
        if t in SKIN_TYPES:
            return False
        if (t in CONCERNS) or (t in CONCERN_SYNONYMS):
            return False
        if (t in CATEGORY_SYNONYMS) or t.endswith("í† ë„ˆ") or (t in {"ì„¸ëŸ¼","ì•°í”Œ","ì—ì„¼ìŠ¤"}):
            return False

    # â€œì¶”ì²œ/ì°¾ì•„ì¤˜/ê³¨ë¼ì¤˜â€ ê°™ì€ ëª…ì‹œì  ìš”ì²­ì–´ë„ ìŠ¤í‚¨ì¼€ì–´ ì˜ë„ë¡œ ê°„ì£¼
    if re.search(r"(ì¶”ì²œ|ì°¾ì•„ì¤˜|ê³¨ë¼ì¤˜|ì¶”ì²œí•´|ì¶”ì²œí•´ì¤˜)", tx):
        return False

    return True


def _messages_to_text(messages, limit=30) -> str:
    out = []
    for m in messages[-limit:]:
        role = "ì‚¬ìš©ì" if isinstance(m, HumanMessage) else "ë„ìš°ë¯¸"
        out.append(f"{role}: {_coerce_to_text(m.content)}")
    return "\n".join(out)

# =========================
# íŒŒì‹±
# =========================
def _rule_based_parse(s: str) -> Dict[str, Any]:
    tokens = _normalize_tokens(s)
    skin = None
    concerns: List[str] = []
    category = None
    for t in tokens:
        if t in SKIN_TYPES:
            skin = t; continue

        if t in CONCERNS:
            concerns.append("ì£¼ë¦„/íƒ„ë ¥" if t in {"ì£¼ë¦„", "íƒ„ë ¥"} else t); continue
        if t in CONCERN_SYNONYMS:
            concerns.append(CONCERN_SYNONYMS[t]); continue

        if ("ë³´ìŠµ" in t) or ("ìˆ˜ë¶„" in t):
            concerns.append("ë³´ìŠµ"); continue
        if "ëª¨ê³µ" in t:
            concerns.append("ëª¨ê³µì¼€ì–´"); continue
        if ("í”¼ì§€" in t) or ("ë²ˆë“¤" in t) or ("ìœ ë¶„" in t) or ("ì—¬ë“œë¦„" in t) or ("íŠ¸ëŸ¬ë¸”" in t):
            concerns.append("í”¼ì§€ì¡°ì ˆ"); continue
        if ("ì§„ì •" in t) or ("ë¶‰" in t) or ("í™ì¡°" in t):
            concerns.append("ì§„ì •"); continue
        if ("ë¯¸ë°±" in t) or ("í†¤ì—…" in t) or ("ì¡í‹°" in t):
            concerns.append("ë¯¸ë°±"); continue
        if ("ì£¼ë¦„" in t) or ("íƒ„ë ¥" in t) or ("ë¦¬í”„íŒ…" in t):
            concerns.append("ì£¼ë¦„/íƒ„ë ¥"); continue

        if t in CATEGORY_SYNONYMS:
            category = CATEGORY_SYNONYMS[t]; continue
        if t.endswith("í† ë„ˆ"):
            category = "ìŠ¤í‚¨/í† ë„ˆ"
        elif t in {"ì„¸ëŸ¼", "ì•°í”Œ", "ì—ì„¼ìŠ¤"}:
            category = "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼"

    # ì¤‘ë³µ ì œê±°
    concerns = list(dict.fromkeys(concerns))

    return {
        "skin_type": skin or "ì•Œ ìˆ˜ ì—†ìŒ",
        "concerns": concerns or ["ì•Œ ìˆ˜ ì—†ìŒ"],
        "category": category or "ì•Œ ìˆ˜ ì—†ìŒ",
    }

def _llm_json_parse(s: str) -> Dict[str, Any]:
    prompt = f"""
ì•„ë˜ ë¬¸ì¥ì—ì„œ ì‚¬ìš©ì ì •ë³´ë¥¼ JSONìœ¼ë¡œë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
- í”¼ë¶€ íƒ€ì…: ë¯¼ê°ì„±, ì§€ì„±, ê±´ì„±, ì•„í† í”¼ì„±, ë³µí•©ì„±, ì¤‘ì„±
- í”¼ë¶€ ê³ ë¯¼: ë³´ìŠµ, ì§„ì •, ë¯¸ë°±, ì£¼ë¦„/íƒ„ë ¥, ëª¨ê³µì¼€ì–´, í”¼ì§€ì¡°ì ˆ
- ì œí’ˆ ì¢…ë¥˜: ìŠ¤í‚¨/í† ë„ˆ, ë¡œì…˜/ì—ë©€ì „, ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼, í¬ë¦¼, ë°¤/ë©€í‹°ë°¤, í´ë Œì§• í¼, ì‹œíŠ¸ë§ˆìŠ¤í¬, ì„ í¬ë¦¼
ëª» ì°¾ìœ¼ë©´ "ì•Œ ìˆ˜ ì—†ìŒ"ìœ¼ë¡œ ì±„ì›Œì£¼ì„¸ìš”.

ë°˜ë“œì‹œ ìˆœìˆ˜ JSONë§Œ:
{{
  "skin_type": "...",
  "concerns": ["..."],
  "category": "..."
}}

ì…ë ¥: {s}
"""
    resp = llm.invoke(prompt).content.strip()
    try:
        if "{" in resp and "}" in resp:
            resp = resp[resp.index("{"): resp.rindex("}") + 1]
        data = json.loads(resp)
    except Exception:
        data = {"skin_type": "ì•Œ ìˆ˜ ì—†ìŒ", "concerns": ["ì•Œ ìˆ˜ ì—†ìŒ"], "category": "ì•Œ ìˆ˜ ì—†ìŒ"}

    if isinstance(data.get("concerns"), str):
        data["concerns"] = [data["concerns"]]
    cat = (data.get("category") or "").strip()
    data["category"] = CATEGORY_SYNONYMS.get(cat, cat if cat else "ì•Œ ìˆ˜ ì—†ìŒ")
    return data

def _infer_prefs_from_history(messages) -> Dict[str, Any]:
    """ì´ì „ ëŒ€í™”ì—ì„œ ê°€ì¥ ìµœê·¼ì— í™•ì •ëœ ì¡°ê±´ì„ JSONìœ¼ë¡œë§Œ ì¶”ì¶œ."""
    history_text = _messages_to_text(messages, limit=30)
    prompt = f"""
ì•„ë˜ ëŒ€í™” ê¸°ë¡ì—ì„œ ê°€ì¥ ìµœê·¼ì— í™•ì •ëœ ì‚¬ìš©ì ì¡°ê±´ì„ JSONìœ¼ë¡œë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
ëª» ì°¾ìœ¼ë©´ "ì•Œ ìˆ˜ ì—†ìŒ"ìœ¼ë¡œ ì±„ì›Œì£¼ì„¸ìš”.

ë°˜ë“œì‹œ ìˆœìˆ˜ JSONë§Œ:
{{
  "skin_type": "...",
  "concerns": ["..."],
  "category": "..."
}}

ëŒ€í™” ê¸°ë¡:
{history_text}
"""
    resp = llm.invoke(prompt).content.strip()
    try:
        if "{" in resp and "}" in resp:
            resp = resp[resp.index("{"): resp.rindex("}") + 1]
        data = json.loads(resp)
    except Exception:
        data = {"skin_type": "ì•Œ ìˆ˜ ì—†ìŒ", "concerns": ["ì•Œ ìˆ˜ ì—†ìŒ"], "category": "ì•Œ ìˆ˜ ì—†ìŒ"}

    if isinstance(data.get("concerns"), str):
        data["concerns"] = [data["concerns"]]
    cat = (data.get("category") or "").strip()
    data["category"] = CATEGORY_SYNONYMS.get(cat, cat if cat else "ì•Œ ìˆ˜ ì—†ìŒ")
    return data

# =========================
# LangGraph ë…¸ë“œ
# =========================
def parse_user_input(state: Dict[str, Any]):
    """
    1) í˜„ì¬ ë¬¸ì¥ ê·œì¹™ íŒŒì‹±
    2) í›„ì†ì§ˆë¬¸(ê°™ì€ ì¡°ê±´/ë„/ë˜/ì—­ì‹œ + ì¹´í…Œê³ ë¦¬)ì¼ ë•ŒëŠ” 'ì´ì „ í™•ì •ê°’'ì„ ê³ ì • ìœ ì§€í•˜ê³  ì¹´í…Œê³ ë¦¬ë§Œ êµì²´
    3) ë¶€ì¡±í•˜ë©´ ê³¼ê±°ëŒ€í™”ë¡œ ë°±í•„ â†’ ê·¸ë˜ë„ ë¶€ì¡±í•˜ë©´ LLM JSON ë³´ì •
    4) 'prefs'ì— ì´ë²ˆ í„´ ì„ íƒê°’ ì €ì¥ (ë‹¤ìŒ í„´ í›„ì†ì§ˆë¬¸ì—ì„œ ì‚¬ìš©)
    """
    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€
    last = ""
    for m in reversed(state.get("messages", [])):
        if isinstance(m, HumanMessage):
            last = _coerce_to_text(m.content)
            break

    # â›” ì˜¤í”„í† í”½(ìŠ¤í‚¨ì¼€ì–´ ì˜ë„ ì „í˜€ ì—†ìŒ)ì¼ ë•ŒëŠ” ê³§ë°”ë¡œ 'ì •ë³´ ë¶€ì¡±'ìœ¼ë¡œ ë°˜í™˜í•´ì„œ
    # ask_for_clarification ë…¸ë“œë¡œ íë¥´ê²Œ ë§Œë“ ë‹¤. (ê¸°ì¡´ prefsëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
    if _looks_offtopic(last):
        return {
            "user_selections": {
                "skin_type": "ì•Œ ìˆ˜ ì—†ìŒ",
                "concerns": ["ì•Œ ìˆ˜ ì—†ìŒ"],
                "category": "ì•Œ ìˆ˜ ì—†ìŒ",
            }
        }  

    # ì§ì „ í™•ì • ì¡°ê±´(ìˆìœ¼ë©´ ìµœìš°ì„ ìœ¼ë¡œ ì‚¬ìš©)
    last_confirmed = state.get("last_confirmed_selections") or {}
    prev_skin = last_confirmed.get("skin_type")
    prev_concerns = last_confirmed.get("concerns", [])
    prev_category = last_confirmed.get("category")

    # 1) ê·œì¹™ ê¸°ë°˜ 1ì°¨ íŒŒì‹±
    parsed = _rule_based_parse(last)

    # ---- í›„ì†ì§ˆë¬¸ ì˜ë„ íƒì§€: "ê°™ì€ ì¡°ê±´", "~ë„/ë˜/ì—­ì‹œ" ----
    txt = last or ""
    followup_signal = bool(re.search(r"(ê°™ì€\s*ì¡°ê±´)|(?:^|\s)(ë„|ë˜|ì—­ì‹œ)(?:\s|$)", txt))

    # ì¹´í…Œê³ ë¦¬ ì˜ë„ ì¶”ì¶œ (switch / add)
    mode, cats = _extract_category_intent(last)
    cat_from_intent = cats[0] if cats else (parsed.get("category") if parsed.get("category") != "ì•Œ ìˆ˜ ì—†ìŒ" else None)

    # ì‚¬ìš©ì ì…ë ¥ì— 'ëª…ì‹œì ' í”¼ë¶€íƒ€ì…/ê³ ë¯¼ì´ ë“¤ì–´ìˆëŠ”ì§€ í™•ì¸
    tokens = _normalize_tokens(txt)
    explicit_skin = any(t in SKIN_TYPES for t in tokens)
    explicit_concern = any(
        (t in CONCERNS) or (t in CONCERN_SYNONYMS)
        or ("ë³´ìŠµ" in t) or ("ìˆ˜ë¶„" in t) or ("ì§„ì •" in t) or ("ë¯¸ë°±" in t)
        or ("ì£¼ë¦„" in t) or ("íƒ„ë ¥" in t) or ("ëª¨ê³µ" in t) or ("í”¼ì§€" in t) or ("ì—¬ë“œë¦„" in t) or ("íŠ¸ëŸ¬ë¸”" in t)
        for t in tokens
    )

    # 2) í›„ì†ì§ˆë¬¸: "ê°™ì€ ì¡°ê±´ìœ¼ë¡œ ~ë„/ë˜/ì—­ì‹œ" + ì¹´í…Œê³ ë¦¬ë§Œ ë§í•œ ê²½ìš° â†’ í”¼ë¶€íƒ€ì…/ê³ ë¯¼ì€ ìœ ì§€, ì¹´í…Œê³ ë¦¬ë§Œ êµì²´
    if followup_signal and cat_from_intent:
        if last_confirmed:
            if not explicit_skin:
                parsed["skin_type"] = prev_skin or parsed.get("skin_type", "ì•Œ ìˆ˜ ì—†ìŒ")
            if not explicit_concern:
                parsed["concerns"] = prev_concerns or parsed.get("concerns", ["ì•Œ ìˆ˜ ì—†ìŒ"])
        parsed["category"] = cat_from_intent

    # 3) ëˆ„ë½ê°’ ë°±í•„(ì´ì „ ëŒ€í™” â†’ LLM JSON ìˆœ)
    if (
        parsed["skin_type"] == "ì•Œ ìˆ˜ ì—†ìŒ"
        or parsed["concerns"] == ["ì•Œ ìˆ˜ ì—†ìŒ"]
        or parsed["category"] == "ì•Œ ìˆ˜ ì—†ìŒ"
    ):
        defaults = _infer_prefs_from_history(state.get("messages", []))
        if parsed["skin_type"] == "ì•Œ ìˆ˜ ì—†ìŒ":
            parsed["skin_type"] = defaults.get("skin_type", "ì•Œ ìˆ˜ ì—†ìŒ")
        if parsed["concerns"] == ["ì•Œ ìˆ˜ ì—†ìŒ"]:
            parsed["concerns"] = defaults.get("concerns", ["ì•Œ ìˆ˜ ì—†ìŒ"])
        if parsed["category"] == "ì•Œ ìˆ˜ ì—†ìŒ":
            parsed["category"] = defaults.get("category", "ì•Œ ìˆ˜ ì—†ìŒ")

    if (
        parsed["skin_type"] == "ì•Œ ìˆ˜ ì—†ìŒ"
        or parsed["concerns"] == ["ì•Œ ìˆ˜ ì—†ìŒ"]
        or parsed["category"] == "ì•Œ ìˆ˜ ì—†ìŒ"
    ):
        fill = _llm_json_parse(last)
        if parsed["skin_type"] == "ì•Œ ìˆ˜ ì—†ìŒ":
            parsed["skin_type"] = fill["skin_type"]
        if parsed["concerns"] == ["ì•Œ ìˆ˜ ì—†ìŒ"]:
            parsed["concerns"] = fill["concerns"]
        if parsed["category"] == "ì•Œ ìˆ˜ ì—†ìŒ":
            parsed["category"] = fill["category"]

    # ì•ˆì „ ë³´ì •
    if not parsed.get("skin_type"):
        parsed["skin_type"] = "ì•Œ ìˆ˜ ì—†ìŒ"
    if not parsed.get("concerns"):
        parsed["concerns"] = ["ì•Œ ìˆ˜ ì—†ìŒ"]
    if not parsed.get("category"):
        parsed["category"] = "ì•Œ ìˆ˜ ì—†ìŒ"

    # ë‹¤ìŒ í„´ìš© ì„ì‹œ ë©”ëª¨
    state["prefs"] = parsed
    return {"user_selections": parsed, "prefs": parsed}


def check_parsing_status(state: Dict[str, Any]):
    """
    âœ… ì™„í™” ê·œì¹™: ì¹´í…Œê³ ë¦¬ê°€ ì—†ì–´ë„ 'í”¼ë¶€íƒ€ì…' ë˜ëŠ” 'ê³ ë¯¼' ì¤‘ í•˜ë‚˜ë§Œ ìˆìœ¼ë©´ ì§„í–‰
    (ì¹´í…Œê³ ë¦¬ ì—†ëŠ” ê²½ìš° find_productsê°€ ì „ ì¹´í…Œê³ ë¦¬ ìŠ¤ìº”ìœ¼ë¡œ ì»¤ë²„)
    """
    s = state["user_selections"]
    has_cat = s.get("category") and s["category"] != "ì•Œ ìˆ˜ ì—†ìŒ"
    has_skin = s.get("skin_type") and s["skin_type"] != "ì•Œ ìˆ˜ ì—†ìŒ"
    has_conc = s.get("concerns") and s["concerns"] != ["ì•Œ ìˆ˜ ì—†ìŒ"]
    return "success" if (has_cat or has_skin or has_conc) else "clarification_needed"

def ask_for_clarification(state: Dict[str, Any]):
    s = state["user_selections"]
    need_cat = (s.get("category") in (None, "", "ì•Œ ìˆ˜ ì—†ìŒ"))
    need_skin = (s.get("skin_type") in (None, "", "ì•Œ ìˆ˜ ì—†ìŒ"))
    need_conc = (not s.get("concerns")) or (s.get("concerns") == ["ì•Œ ìˆ˜ ì—†ìŒ"])
    missing = []
    if need_cat: missing.append("ì œí’ˆ ì¢…ë¥˜")
    if need_skin and need_conc: missing.append("í”¼ë¶€ íƒ€ì… ë˜ëŠ” í”¼ë¶€ ê³ ë¯¼")
    text = "ì§ˆë¬¸ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì •í™•í•œ ì¶”ì²œì„ ìœ„í•´ ì œí’ˆ ì¢…ë¥˜ì™€ í”¼ë¶€ íƒ€ì… ë˜ëŠ” ê³ ë¯¼ ì¤‘ í•˜ë‚˜ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. ğŸ™‚ ì˜ˆ: ì§€ì„±, ë¡œì…˜ / ê±´ì„±, ë³´ìŠµ, í¬ë¦¼"
    return {"messages": [AIMessage(content=text)]}


def get_ingredients(state: Dict[str, Any]):
    s = state["user_selections"]
    skin_type = s.get("skin_type", "ì•Œ ìˆ˜ ì—†ìŒ")
    concerns = s.get("concerns", ["ì•Œ ìˆ˜ ì—†ìŒ"])
    skin_label = skin_type if skin_type != "ì•Œ ìˆ˜ ì—†ìŒ" else "ì¼ë°˜ì ì¸"

    if concerns == ["ì•Œ ìˆ˜ ì—†ìŒ"]:
        prompt_text = f"""
        ì—­í• : í™”ì¥í’ˆ ì„±ë¶„ íë ˆì´í„°.
        ëª©í‘œ: '{skin_label}' í”¼ë¶€ì— ë³´í¸ì ìœ¼ë¡œ ì•ˆì „í•˜ê³  ìœ íš¨í•œ í•µì‹¬ í™œì„± ì„±ë¶„ 5ê°œë§Œ ì„ ì •.
        ì§€ì¹¨: ìê·¹ ë‚®ê³  ê·¼ê±° ê¸°ë°˜. ë³´ì¡°/ìš©ë§¤/í–¥/ë³´ì¡´ì œ/UVí•„í„° ì œì™¸.
        ì¶œë ¥: ì‰¼í‘œë¡œë§Œ êµ¬ë¶„ëœ í•œ ì¤„
        """
    else:
        prompt_text = f"""
        ì—­í• : í™”ì¥í’ˆ ì„±ë¶„ íë ˆì´í„°.
        ëª©í‘œ: '{skin_label}' í”¼ë¶€ì˜ '{', '.join(concerns)}' ê³ ë¯¼ ê°œì„ ì— ê¸°ì—¬í•˜ëŠ” í•µì‹¬ í™œì„± ì„±ë¶„ 5ê°œë§Œ ì„ ì •.
        ì§€ì¹¨: ê·¼ê±° ê¸°ë°˜ í™œì„± ìœ„ì£¼, ë³´ì¡°/ìš©ë§¤/í–¥/ë³´ì¡´ì œ/UVí•„í„° ì œì™¸.
        ì¶œë ¥: ì‰¼í‘œë¡œë§Œ êµ¬ë¶„ëœ í•œ ì¤„
        """
    resp = llm.invoke(prompt_text)
    key_ingredients_str = resp.content.strip()
    return {"key_ingredients": [ing.strip().lower() for ing in key_ingredients_str.split(",") if ing.strip()]}

from pathlib import Path
DATA_PATH = Path(__file__).parent / "product_data.csv"

def find_products(state: Dict[str, Any]):
    """
    - ì¹´í…Œê³ ë¦¬ê°€ ìˆìœ¼ë©´: í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ìƒìœ„ ê²°ê³¼
    - ì¹´í…Œê³ ë¦¬ê°€ ì—†ìœ¼ë©´: ì „ ì¹´í…Œê³ ë¦¬ë¥¼ í›‘ì–´ ì¹´í…Œê³ ë¦¬ë³„ í›„ë³´ 1ê°œì”© ìˆ˜ì§‘ â†’ ìƒìœ„ 3ê°œ
    """
    sel = state["user_selections"]
    key_ings = state.get("key_ingredients", [])
    cat = sel.get("category")

    # ì¹´í…Œê³ ë¦¬ ì§€ì • O â†’ ë‹¨ì¼ ì¹´í…Œê³ ë¦¬
    if cat and cat != "ì•Œ ìˆ˜ ì—†ìŒ":
        top = find_and_rank_products(str(DATA_PATH), sel, key_ings) or []
        return {"top_products": top}

    # ì¹´í…Œê³ ë¦¬ ì§€ì • X â†’ ì „ ì¹´í…Œê³ ë¦¬ ìŠ¤ìº”
    bucket = []
    for c in ALL_CATEGORIES:
        sub_sel = {**sel, "category": c}
        items = find_and_rank_products(str(DATA_PATH), sub_sel, key_ings) or []
        if items:
            bucket.append(items[0])
    # ìƒìœ„ 3ê°œë§Œ ë…¸ì¶œ(ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
    return {"top_products": bucket[:3]}

# [ADD] ì œí’ˆë³„ 'ì¶”ì²œ ì´ìœ ' ì›¹ ìš”ì•½ (ë¶€ì¡±í•˜ë©´ ì„±ë¶„ ê¸°ë°˜ í´ë°±)
def _fetch_reasons_for_products(products: List[dict], selections: Dict[str, Any], key_ingredients: List[str]) -> List[str]:
    reasons: List[str] = []
    skin = selections.get("skin_type", "ì•Œ ìˆ˜ ì—†ìŒ")
    concerns = ", ".join([c for c in selections.get("concerns", []) if c and c != "ì•Œ ìˆ˜ ì—†ìŒ"]) or "ì•Œ ìˆ˜ ì—†ìŒ"
    category = selections.get("category", "ì•Œ ìˆ˜ ì—†ìŒ")

    for p in products[:3]:
        brand = (p.get("brand") or p.get("ë¸Œëœë“œëª…") or "").strip()
        name = (p.get("name") or p.get("ì œí’ˆëª…") or "").strip()

        matched = ", ".join(sorted(set([m for m in (p.get("found_ingredients") or []) if m]))) \
                  or ", ".join([k for k in (key_ingredients or []) if k])

        query = f"{brand} {name} ì„±ë¶„ íš¨ê³¼ ë¦¬ë·° ì¥ë‹¨ì  {category} {skin} {concerns}"
        web_results = _search_prefer(query)

        prompt = f"""
ì—­í• : ë‹¹ì‹ ì€ í™”ì¥í’ˆ ì¶”ì²œ ê·¼ê±° ìš”ì•½ê°€ì…ë‹ˆë‹¤.
ìƒí™©: ì‚¬ìš©ìëŠ” {skin} í”¼ë¶€, ê³ ë¯¼ì€ {concerns}, ì¹´í…Œê³ ë¦¬ëŠ” {category}ì…ë‹ˆë‹¤.
ì œí’ˆ: {brand} {name}
ë§¤ì¹­/í•µì‹¬ ì„±ë¶„: {matched}

ìë£Œ(ì›¹ ê²€ìƒ‰ ìŠ¤ë‹ˆí«):
---
{web_results}
---

ê·œì¹™:
- 'ì™œ ì´ ì œí’ˆì„ ì¶”ì²œí•˜ëŠ”ì§€' í•œ ì¤„(35~60ì)ë¡œ í•œêµ­ì–´ ìš”ì•½
- ê°€ëŠ¥í•œ ê·¼ê±°: ë§¤ì¹­ ì„±ë¶„ íš¨ëŠ¥, ì„ìƒ/ë³´ìŠµ/ì§„ì • ì§€í‘œ, ì €ìê·¹(ë¬´í–¥/ì•½ì‚°ì„±), ë…¼ë€ ì„±ë¶„ ë¬´ì²¨ê°€ ë“±
- ìë£Œì— ì—†ëŠ” ìˆ˜ì¹˜/ì‚¬ì‹¤ ì°½ì‘ ê¸ˆì§€
- ìë£Œ ë¶€ì¡± ì‹œ ë§¤ì¹­ ì„±ë¶„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±
- ì¶œë ¥: í•œ ì¤„ë§Œ, ë¶ˆë¦¿/ë¨¸ë¦¬ê¸°í˜¸/ë”°ì˜´í‘œ ì—†ì´, ë§ˆì¹¨í‘œ ì—†ì´
"""
        try:
            resp = llm.invoke(prompt)
            reason = (resp.content or "").strip().splitlines()[0]
        except Exception:
            reason = ""

        import re as _re
        reason = _re.sub(r"^[â€¢\-\*\d\.\)\s]+", "", reason) or "í•µì‹¬ ì„±ë¶„ê³¼ ì €ìê·¹ ì§€í‘œê°€ ì¡°ê±´ì— ë¶€í•©"
        reasons.append(reason)

    return reasons

# --- [ADD] found_ingredientsê°€ ë¹„ì—ˆì„ ë•Œ, ì›¹ ìŠ¤ë‹ˆí«ìœ¼ë¡œ íš¨ëŠ¥ ì„±ë¶„ì„ 3~6ê°œ ì¶”ì¶œí•˜ëŠ” í´ë°± ---
def infer_beneficial_ings_via_web(brand: str, name: str, fallback_key_ings: List[str]) -> List[str]:
    import re
    query = f"{brand} {name} ì „ì„±ë¶„ íš¨ëŠ¥ ì„±ë¶„ ì„±ë¶„í‘œ ì„±ë¶„ ë¦¬ìŠ¤íŠ¸"
    # âœ… í•¨ìˆ˜ëª… ì˜¤íƒ€ ìˆ˜ì •: search_prefer â†’ _search_prefer
    web_results = _search_prefer(query)

    prompt = f"""
    ì—­í• : ë‹¹ì‹ ì€ í™”ì¥í’ˆ ì„±ë¶„ íë ˆì´í„°ì…ë‹ˆë‹¤.
    ì•„ë˜ ìë£Œ(ì›¹ ìŠ¤ë‹ˆí«)ì—ì„œ '{brand} {name}' ì œí’ˆì˜ í”¼ë¶€ì— ì´ë“ì´ ë˜ëŠ” 'í•µì‹¬ íš¨ëŠ¥ ì„±ë¶„'ë§Œ 3~6ê°œ í•œêµ­ì–´ ì„±ë¶„ëª…ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.
    - ë³´ìŠµ/ì§„ì •/ë¯¸ë°±/ì£¼ë¦„/ëª¨ê³µ/í”¼ì§€ ë“±ê³¼ ì§ì ‘ ê´€ë ¨ëœ í™œì„± ì„±ë¶„ ìœ„ì£¼
    - ìš©ë§¤/ë³´ì¡´ì œ/í–¥ë£Œ/ê°€êµì œ ë“± ë³´ì¡° ì„±ë¶„ ì œì™¸
    - ì¶œë ¥ì€ ì‰¼í‘œë¡œë§Œ ë‚˜ì—´ (ì˜ˆ: íˆì•Œë£¨ë¡ ì‚°, ì„¸ë¼ë§ˆì´ë“œ, ë‚˜ì´ì•„ì‹ ì•„ë§ˆì´ë“œ)

    ìë£Œ:
    ---
    {web_results}
    ---
    """
    try:
        resp = llm.invoke(prompt)
        txt = (resp.content or "").strip()
        # ì¤„ë°”ê¿ˆìœ¼ë¡œ ì˜¤ëŠ” ê²½ìš°ë„ ëŒ€ë¹„í•´ì„œ ì‰¼í‘œ/ì¤„ë°”ê¿ˆì„ ëª¨ë‘ ë¶„ë¦¬
        raw = [x.strip() for x in re.split(r"[,\n]", txt) if x.strip()]
        filtered = []
        for x in raw:
            # âŒ ì‚¬ê³¼/ì„¤ëª… ë¬¸êµ¬ ì œê±°
            if any(bad in x for bad in ["ì£„ì†¡", "ì¶”ì¶œí•  ìˆ˜ ì—†", "ì œê³µëœ ìë£Œ", "ì •ë³´ê°€ ë¶€ì¡±", "ì—†ìŠµë‹ˆë‹¤"]):
                continue
            # ë„ˆë¬´ ê¸´ ë¬¸ì¥/ë‹¨ë½ì„± í…ìŠ¤íŠ¸ ì œê±° (ì„±ë¶„ëª…ì´ ì•„ë‹Œ ê²½ìš°)
            if len(x) > 28 or len(x.split()) > 5:
                continue
            filtered.append(x)
        ings = list(dict.fromkeys(filtered))[:6]
    except Exception:
        ings = []

    # ì™„ì „ ì‹¤íŒ¨ ì‹œ, ë¶„ì„ ê¸°ì¤€ ì„±ë¶„ìœ¼ë¡œ í´ë°±
    if not ings and fallback_key_ings:
        ings = list(dict.fromkeys([k.strip() for k in fallback_key_ings if k and str(k).strip()]))[:6]
    return ings


# [ADD] í•˜ë‹¨ 'ì£¼ì˜ ì„±ë¶„' ìƒì„± (íš¨ëŠ¥ ì„±ë¶„ê³¼ ê²¹ì¹˜ë©´ [ì¡°ê±´ë¶€]ë¡œ í‘œê¸°)
def _fetch_warnings_for_ingredients(ingredients: List[str], efficacy_ings: List[str] = None) -> str:
    if not ingredients:
        return ""
    efficacy_ings = [str(i or "").lower().strip() for i in (efficacy_ings or []) if str(i or "").strip()]

    query = f"{', '.join(ingredients)} í™”ì¥í’ˆ ìœ í•´ì„± ì£¼ì˜ì‚¬í•­"
    web_results = _search_prefer(query)

    prompt = f"""
ì—­í• : ë‹¹ì‹ ì€ í™”ì¥í’ˆ ì•ˆì „ì„± ìš”ì•½ê°€ì…ë‹ˆë‹¤.
ìë£Œ: ì•„ë˜ëŠ” ì„±ë¶„ ìœ„í—˜ì„± ê´€ë ¨ ì›¹ ê²€ìƒ‰ ìŠ¤ë‹ˆí«ì…ë‹ˆë‹¤.
---
{web_results}
---
ì…ë ¥ ì„±ë¶„(í›„ë³´): {', '.join(ingredients)}
íš¨ëŠ¥ ì„±ë¶„(ê²¹ì¹˜ë©´ ê¸°ë³¸ [ì¡°ê±´ë¶€]): {', '.join(efficacy_ings)}
ì¼ë°˜ ì•ˆì „/ë³´ìŠµ ì„±ë¶„(íŠ¹ë³„ ê·¼ê±° ì—†ìœ¼ë©´ ì œì™¸): {', '.join(SAFE_BENIGN_INGS)}

ì§€ì¹¨:
1) 'í›„ë³´' ì¤‘ ë‹¤ìŒ ê¸°ì¤€ë§Œ ê²½ê³ :
   - ì•Œë ˆë¥´ê¸°/ìê·¹ ë³´ê³  ë¹ˆë„â†‘(í–¥ë£Œ/ì—ì„¼ì…œì˜¤ì¼, ë¦¬ëª¨ë„¨/ë¦¬ë‚ ë£°/ìœ ì œë†€ ë“±)
   - ì‚°/ë ˆí‹°ë…¸ì´ë“œ/ë²¤ì¡°ì¼í¼ì˜¥ì‚¬ì´ë“œ ë“± ê³ ë†ë„Â·pH ì˜ì¡´ ìê·¹ ê°€ëŠ¥
   - ë…¼ìŸì„± UV í•„í„°(ì˜¥ì‹œë²¤ì¡´/ì˜¥í‹°ë…¹ì„¸ì´íŠ¸ ë“±), í¬ë¦„ì•Œë°í•˜ì´ë“œ ë°©ì¶œ ë°©ë¶€ì œ ë“±
2) íš¨ëŠ¥ ì„±ë¶„ê³¼ ê²¹ì¹˜ë©´ ê¸°ë³¸ [ì¡°ê±´ë¶€]ë¡œ í‘œê¸°í•˜ê³  ì‚¬ìœ ë¥¼ 25ì ì´ë‚´ë¡œ
3) 'ì¼ë°˜ ì•ˆì „/ë³´ìŠµ' ë¦¬ìŠ¤íŠ¸ëŠ” íŠ¹ë³„í•œ ê·¼ê±° ì—†ìœ¼ë©´ ì œì™¸
4) ì¤‘ë³µ ì œê±°, 3~5ê°œ ì´ë‚´, ì¤‘ìš”ë„ ìˆœ
ì¶œë ¥(ì´ í˜•ì‹ë§Œ):
- ì„±ë¶„ â€” [ìœ„í—˜|ì¡°ê±´ë¶€] í•œì¤„ ì´ìœ 
- ì„±ë¶„ â€” [ìœ„í—˜|ì¡°ê±´ë¶€] í•œì¤„ ì´ìœ 
"""
    try:
        resp = llm.invoke(prompt)
        txt = (resp.content or "").strip()
    except Exception:
        txt = ""

    # 5ì¤„ ì œí•œ & ë¶ˆë¦¿ ì •ë¦¬
    lines = [ln.strip() for ln in txt.splitlines() if ln.strip()]
    if len(lines) > 5:
        lines = lines[:5]
    return "\n".join(lines)


def create_recommendation_message(state: Dict[str, Any]):
    """
    ê³ ì • í¬ë§·ìœ¼ë¡œ ìµœì¢… ë©”ì‹œì§€ë¥¼ ì¡°ë¦½í•©ë‹ˆë‹¤.
    - ìƒë‹¨: íš¨ëŠ¥ ì„±ë¶„(ë¶„ì„ ê¸°ì¤€)
    - ì œí’ˆ 1~3ìœ„ ì¹´ë“œ:
        - ë¸Œëœë“œ / ê°€ê²© / ìš©ëŸ‰ / ğŸ”— [ë§í¬]
        - ğŸ§ª íš¨ëŠ¥ ì„±ë¶„  â† (ë§¤ì¹­ ì„±ë¶„ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ì „ì„±ë¶„ì—ì„œ key_ingredients êµì°¨ê²€ì¶œ)
        - âœ… ì¶”ì²œ ì´ìœ (ì›¹ ìš”ì•½)
        - âš ï¸ ì£¼ì˜ ì„±ë¶„  â† (ì œí’ˆë³„, íš¨ëŠ¥ ì„±ë¶„ê³¼ ê²¹ì¹˜ë©´ ì œì™¸, [ì¡°ê±´ë¶€] í‘œê¸° ì—†ìŒ)
    """
    import re
    from langchain_core.messages import AIMessage

    top = state.get("top_products", []) or []
    if not top:
        text = "ì¡°ê±´ì— ë§ëŠ” ì œí’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¡°ê±´ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ ë³´ì‹¤ë˜ìš”?"
        return {"messages": [AIMessage(content=text)], "recommendation_message": text}

    # í—¤ë”
    lines: List[str] = []
    lines.append("ìš”ì²­í•˜ì‹  ì¡°ê±´ì— ë§ì¶° ì¶”ì²œ ì œí’ˆì„ ì •ë¦¬í–ˆì–´ìš”. ğŸ˜Š")

    s = state.get("user_selections", {}) or {}
    skin = s.get("skin_type", "ì•Œ ìˆ˜ ì—†ìŒ")
    concerns_list = [c for c in s.get("concerns", []) if c and c != "ì•Œ ìˆ˜ ì—†ìŒ"]
    category = s.get("category", "ì•Œ ìˆ˜ ì—†ìŒ")

    cond_bits = []
    if skin != "ì•Œ ìˆ˜ ì—†ìŒ":
        cond_bits.append(f"ğŸ§‘â€ğŸ¦° í”¼ë¶€: **{skin}**")
    if concerns_list:
        cond_bits.append(f"ğŸŒ¿ ê³ ë¯¼: **{', '.join(concerns_list)}**")
    if category != "ì•Œ ìˆ˜ ì—†ìŒ":
        cond_bits.append(f"ğŸ§´ ì œí’ˆ: **{category}**")
    if cond_bits:
        lines.append("**ğŸ¯ ì‚¬ìš©ì ì¡°ê±´**")
        lines.append("   " + "   Â·   ".join(cond_bits))
        lines.append("")

    # íš¨ëŠ¥ ì„±ë¶„(ë¶„ì„ ê¸°ì¤€)
    key_ings = [k.strip() for k in state.get("key_ingredients", []) if k and str(k).strip()]
    if key_ings:
        lines.append(f"**ğŸ§ª íš¨ëŠ¥ ì„±ë¶„(ë¶„ì„ ê¸°ì¤€):** {', '.join(key_ings)}")
        lines.append("")

    # ì œí’ˆë³„ 'ì¶”ì²œ ì´ìœ '
    selections = state.get("user_selections", {})
    web_reasons = _fetch_reasons_for_products(top, selections, key_ings)

    # ì œí’ˆ ì¹´ë“œ
    medals = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"]
    safe_set = {s.lower() for s in SAFE_BENIGN_INGS}

    # 2) ì œí’ˆ ì¹´ë“œ (ìƒìœ„ 3ê°œ)
    for i, p in enumerate(top[:3]):
        emoji = medals[i] if i < len(medals) else f"{i+1}."
        name = (p.get("name") or "").strip()
        brand = (p.get("brand") or "").strip()
        price = p.get("price", "?")
        volume = p.get("volume", "?")
        link = (p.get("link") or "").strip()
        link_md = f"[ë§í¬]({link})" if link else "-"

        # --- ì¶”ì²œ ì´ìœ  (ì´ë¯¸ ê³„ì‚°ëœ web_reasons ì‚¬ìš©) ---
        reason = web_reasons[i] if i < len(web_reasons) else "í•µì‹¬ ì„±ë¶„ê³¼ ì €ìê·¹ ì§€í‘œê°€ ì¡°ê±´ì— ë¶€í•©"

        # --- íš¨ëŠ¥ ì„±ë¶„: found_ingredients â†’ ë¹„ì—ˆìœ¼ë©´ ì›¹ í´ë°± ---
        found = [m.strip() for m in p.get("found_ingredients", []) if m and str(m).strip()]
        if not found:
            # ì›¹ì—ì„œ 3~6ê°œ ì¶”ì¶œ + ë§ˆì§€ë§‰ ì•ˆì „ë§ìœ¼ë¡œ key_ingredients ì‚¬ìš©
            found = infer_beneficial_ings_via_web(brand, name, state.get("key_ingredients", []))

        eff_unique_list = sorted(set([x for x in found if x]))
        eff_unique = ", ".join(eff_unique_list) if eff_unique_list else "ì •ë³´ ë¶€ì¡±"

        # --- ì œí’ˆë³„ ì£¼ì˜ ì„±ë¶„: íš¨ëŠ¥ ì„±ë¶„ê³¼ ê²¹ì¹˜ë©´ ì œì™¸ ---
        caution_lines = _fetch_warnings_for_ingredients(eff_unique_list) if eff_unique_list else ""
        caution_items = []
        if caution_lines:
            for ln in [ln.strip() for ln in caution_lines.splitlines() if ln.strip()]:
                token = ln.split("â€”", 1)[0].replace("-", "").strip()
                if token and token not in eff_unique_list and token not in caution_items:
                    caution_items.append(token)
        caution_text = ", ".join(caution_items) if caution_items else "ì—†ìŒ"

        # --- ì¶œë ¥ ---
        lines.append(f"{emoji} {i+1}. {name}")
        lines.append(f"   ğŸ¬ ë¸Œëœë“œ: {brand or '-'}")
        lines.append(f"   ğŸ’° ê°€ê²©: {price}ì›")
        lines.append(f"   ğŸ«™ ìš©ëŸ‰: {volume}")
        lines.append(f"   ğŸ”— {link_md}")
        lines.append(f"   âœ… ì¶”ì²œ ì´ìœ : {reason}")
        lines.append(f"   ğŸ§ª íš¨ëŠ¥ ì„±ë¶„: {eff_unique}")
        lines.append(f"   âš ï¸ ì£¼ì˜ ì„±ë¶„: {caution_text}")
        if i < 2:
            lines.append("")

    final_text = "\n".join(lines)
    return {
    "messages": [AIMessage(content=final_text)],
    "recommendation_message": final_text,
    "last_confirmed_selections": s 
}


import re
import json
from typing import Dict, Any, List

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage

from utils import find_and_rank_products
from langchain_community.tools.tavily_search import TavilySearchResults

# ===== ëª¨ë¸ & ì›¹ê²€ìƒ‰ =====
llm = ChatOpenAI(model_name="gpt-4o", temperature=0)
search = TavilySearchResults(k=3)

# ===== ë™ì˜ì–´/ì •ê·œí™” =====
SKIN_TYPES = {"ì§€ì„±", "ê±´ì„±", "ë³µí•©ì„±", "ë¯¼ê°ì„±", "ì•„í† í”¼ì„±"}
CONCERNS = {"ë³´ìŠµ", "ì§„ì •", "ë¯¸ë°±", "ì£¼ë¦„/íƒ„ë ¥", "ëª¨ê³µì¼€ì–´", "í”¼ì§€ì¡°ì ˆ", "ì£¼ë¦„", "íƒ„ë ¥"}
CATEGORY_SYNONYMS = {
    # ìŠ¤í‚¨/í† ë„ˆ
    "í† ë„ˆ": "ìŠ¤í‚¨/í† ë„ˆ",
    "ìŠ¤í‚¨": "ìŠ¤í‚¨/í† ë„ˆ",
    "ìŠ¤í‚¨/í† ë„ˆ": "ìŠ¤í‚¨/í† ë„ˆ",

    # ë¡œì…˜/ì—ë©€ì „(ì—ë©€ì ¼ í‘œê¸°ë„ í¡ìˆ˜)
    "ë¡œì…˜": "ë¡œì…˜/ì—ë©€ì „",
    "ì—ë©€ì „": "ë¡œì…˜/ì—ë©€ì „",
    "ì—ë©€ì ¼": "ë¡œì…˜/ì—ë©€ì „",
    "ë¡œì…˜/ì—ë©€ì „": "ë¡œì…˜/ì—ë©€ì „",
    "ë¡œì…˜/ì—ë©€ì ¼": "ë¡œì…˜/ì—ë©€ì „",

    # ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼
    "ì„¸ëŸ¼": "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼",
    "ì•°í”Œ": "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼",
    "ì—ì„¼ìŠ¤": "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼",
    "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼": "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼",

    # í¬ë¦¼ & ë°¤
    "í¬ë¦¼": "í¬ë¦¼",
    "ë°¤": "ë°¤/ë©€í‹°ë°¤",
    "ë©€í‹°ë°¤": "ë°¤/ë©€í‹°ë°¤",
    "ë°¤/ë©€í‹°ë°¤": "ë°¤/ë©€í‹°ë°¤",

    # í´ë Œì§•
    "í´ë Œì§•í¼": "í´ë Œì§• í¼",
    "í´ë Œì§•": "í´ë Œì§• í¼",
    "í´ë Œì§• í¼": "í´ë Œì§• í¼",

    # ë§ˆìŠ¤í¬
    "ì‹œíŠ¸ë§ˆìŠ¤í¬": "ì‹œíŠ¸ë§ˆìŠ¤í¬",

    # ì„ ì¼€ì–´ (ì´ì œ 'ì„ í¬ë¦¼'ìœ¼ë¡œ í†µì¼)
    "ì„ í¬ë¦¼": "ì„ í¬ë¦¼",
    "ì„ ë¡œì…˜": "ì„ í¬ë¦¼",
    "ìì™¸ì„ ì°¨ë‹¨ì œ": "ì„ í¬ë¦¼",
    "ìì°¨": "ì„ í¬ë¦¼",
}

ALL_CATEGORIES = ["ìŠ¤í‚¨/í† ë„ˆ","ë¡œì…˜/ì—ë©€ì „","ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼","í¬ë¦¼","ë°¤/ë©€í‹°ë°¤","í´ë Œì§• í¼","ì‹œíŠ¸ë§ˆìŠ¤í¬","ì„ í¬ë¦¼"]

# ì•ˆì „ ì„±ë¶„ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸(ê²¹ì¹¨ ë°©ì§€ìš©) ì¶”ê°€
SAFE_BENIGN_INGS = [
    "ê¸€ë¦¬ì„¸ë¦°","íˆì•Œë£¨ë¡ ì‚°","ì†Œë“í•˜ì´ì•Œë£¨ë¡œë„¤ì´íŠ¸","í•˜ì´ì•Œë£¨ë¡œë‹‰ì• ì”¨ë“œ",
    "ì„¸ë¼ë§ˆì´ë“œ","ì„¸ë¼ë§ˆì´ë“œì—”í”¼","íŒí…Œë†€","ìŠ¤ì¿ ì•Œë€","ë² íƒ€ì¸","ì•Œë€í† ì¸",
    "í† ì½”í˜ë¡¤","ì”íƒ„ê²€","í”„ë¡œíŒë‹¤ì´ì˜¬","ë¶€í‹¸ë Œê¸€ë¼ì´ì½œ","íœí‹¸ë Œê¸€ë¼ì´ì½œ",
    "í•˜ì´ë“œë¡ì‹œì•„ì„¸í† í˜ë…¼","ë‹¤ì´ì†Œë“ì´ë””í‹°ì—ì´"
]

# ===== ê³µí†µ ìœ í‹¸ =====
def _coerce_to_text(x) -> str:
    if isinstance(x, str):
        return x
    if isinstance(x, list):
        parts = []
        for item in x:
            if isinstance(item, dict) and item.get("type") == "text":
                parts.append(item.get("text", ""))
            elif isinstance(item, str):
                parts.append(item)
        return " ".join(p for p in parts if p).strip()
    return str(x)

def _normalize_tokens(text: str) -> List[str]:
    if not isinstance(text, str):
        text = _coerce_to_text(text)
    cleaned = re.sub(r"[\"'`]+", "", text)
    raw = re.split(r"[,/]\s*|\s+", cleaned)
    return [t.strip() for t in raw if t.strip()]

def _messages_to_text(messages, limit=30) -> str:
    """ìµœê·¼ ë©”ì‹œì§€ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ë³‘í•©(ì»¨í…ìŠ¤íŠ¸ ì¶”ë¡ ìš©)."""
    out = []
    for m in messages[-limit:]:
        role = "ì‚¬ìš©ì" if isinstance(m, HumanMessage) else "ë„ìš°ë¯¸"
        out.append(f"{role}: {_coerce_to_text(m.content)}")
    return "\n".join(out)

# ===== íŒŒì‹± ë¡œì§ =====
def _rule_based_parse(s: str) -> Dict[str, Any]:
    tokens = _normalize_tokens(s)
    skin = None
    concerns: List[str] = []
    category = None
    for t in tokens:
        if t in SKIN_TYPES:
            skin = t; continue
        if t in CONCERNS:
            concerns.append("ì£¼ë¦„/íƒ„ë ¥" if t in {"ì£¼ë¦„", "íƒ„ë ¥"} else t); continue
        if t in CATEGORY_SYNONYMS:
            category = CATEGORY_SYNONYMS[t]; continue
        if t.endswith("í† ë„ˆ"):
            category = "ìŠ¤í‚¨/í† ë„ˆ"
        elif t in {"ì„¸ëŸ¼", "ì•°í”Œ", "ì—ì„¼ìŠ¤"}:
            category = "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼"

    return {
        "skin_type": skin or "ì•Œ ìˆ˜ ì—†ìŒ",
        "concerns": concerns or ["ì•Œ ìˆ˜ ì—†ìŒ"],
        "category": category or "ì•Œ ìˆ˜ ì—†ìŒ",
    }

def _llm_json_parse(s: str) -> Dict[str, Any]:
    prompt = f"""
ì•„ë˜ ë¬¸ì¥ì—ì„œ ì‚¬ìš©ì ì •ë³´ë¥¼ JSONìœ¼ë¡œë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
- í”¼ë¶€ íƒ€ì…: ë¯¼ê°ì„±, ì§€ì„±, ê±´ì„±, ì•„í† í”¼ì„±, ë³µí•©ì„±
- í”¼ë¶€ ê³ ë¯¼: ë³´ìŠµ, ì§„ì •, ë¯¸ë°±, ì£¼ë¦„/íƒ„ë ¥, ëª¨ê³µì¼€ì–´, í”¼ì§€ì¡°ì ˆ
- ì œí’ˆ ì¢…ë¥˜: ìŠ¤í‚¨/í† ë„ˆ, ë¡œì…˜/ì—ë©€ì ¼, ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼, í¬ë¦¼, ë°¤/ë©€í‹°ë°¤, í´ë Œì§• í¼, ì‹œíŠ¸ë§ˆìŠ¤í¬, ì„ í¬ë¦¼/ë¡œì…˜
ëª» ì°¾ìœ¼ë©´ "ì•Œ ìˆ˜ ì—†ìŒ"ìœ¼ë¡œ ì±„ì›Œì£¼ì„¸ìš”.

ë°˜ë“œì‹œ ìˆœìˆ˜ JSONë§Œ:
{{
  "skin_type": "...",
  "concerns": ["..."],
  "category": "..."
}}

ì…ë ¥: {s}
"""
    resp = llm.invoke(prompt).content.strip()
    try:
        if "{" in resp and "}" in resp:
            resp = resp[resp.index("{"): resp.rindex("}") + 1]
        data = json.loads(resp)
    except Exception:
        data = {"skin_type": "ì•Œ ìˆ˜ ì—†ìŒ", "concerns": ["ì•Œ ìˆ˜ ì—†ìŒ"], "category": "ì•Œ ìˆ˜ ì—†ìŒ"}

    if isinstance(data.get("concerns"), str):
        data["concerns"] = [data["concerns"]]
    cat = (data.get("category") or "").strip()
    data["category"] = CATEGORY_SYNONYMS.get(cat, cat if cat else "ì•Œ ìˆ˜ ì—†ìŒ")
    return data

def _infer_prefs_from_history(messages) -> Dict[str, Any]:
    """ì´ì „ ëŒ€í™”ì—ì„œ ê°€ì¥ ìµœê·¼ì— í™•ì •ëœ ì¡°ê±´ì„ JSONìœ¼ë¡œ ì¶”ì¶œ."""
    history_text = _messages_to_text(messages, limit=30)
    prompt = f"""
ì•„ë˜ ëŒ€í™” ê¸°ë¡ì—ì„œ ê°€ì¥ ìµœê·¼ì— í™•ì •ëœ ì‚¬ìš©ì ì¡°ê±´ì„ JSONìœ¼ë¡œë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
ëª» ì°¾ìœ¼ë©´ "ì•Œ ìˆ˜ ì—†ìŒ"ìœ¼ë¡œ ì±„ì›Œì£¼ì„¸ìš”.

ë°˜ë“œì‹œ ìˆœìˆ˜ JSONë§Œ:
{{
  "skin_type": "...",
  "concerns": ["..."],
  "category": "..."
}}

ëŒ€í™” ê¸°ë¡:
{history_text}
"""
    resp = llm.invoke(prompt).content.strip()
    try:
        if "{" in resp and "}" in resp:
            resp = resp[resp.index("{"): resp.rindex("}") + 1]
        data = json.loads(resp)
    except Exception:
        data = {"skin_type": "ì•Œ ìˆ˜ ì—†ìŒ", "concerns": ["ì•Œ ìˆ˜ ì—†ìŒ"], "category": "ì•Œ ìˆ˜ ì—†ìŒ"}

    if isinstance(data.get("concerns"), str):
        data["concerns"] = [data["concerns"]]
    cat = (data.get("category") or "").strip()
    data["category"] = CATEGORY_SYNONYMS.get(cat, cat if cat else "ì•Œ ìˆ˜ ì—†ìŒ")
    return data

PREFERRED_SITES = ["hwahae.co.kr"]

def search_prefer(query: str):
    """ì„ í˜¸ ë„ë©”ì¸ â†’ ê²°ê³¼ ì—†ìœ¼ë©´ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±."""
    for site in PREFERRED_SITES:
        try:
            r = search.run(f"site:{site} {query}")
            # ë¬¸ìì—´/ë¦¬ìŠ¤íŠ¸ ëª¨ë‘ ì»¤ë²„: ë‚´ìš© ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
            if (isinstance(r, str) and r.strip()) or (isinstance(r, list) and len(r) > 0):
                return r
        except Exception:
            pass
    return search.run(query)

# ===== ì›¹ ê²€ìƒ‰(ì£¼ì˜ ì„±ë¶„) =====
def fetch_warnings_for_ingredients(ingredients: List[str], efficacy_ings: List[str] = None) -> str:
    """ìœ í•´ ê°€ëŠ¥ ì„±ë¶„ë“¤ì— ëŒ€í•´ ì›¹ ê²€ìƒ‰ í›„ ìš”ì•½ (íš¨ëŠ¥ ì„±ë¶„ê³¼ì˜ ê²¹ì¹¨ì€ ì›ì¹™ì  ì œì™¸/ì¡°ê±´ë¶€ ì²˜ë¦¬)."""
    if not ingredients:
        return ""
    efficacy_ings = efficacy_ings or []

    query = f"{', '.join(ingredients)} í™”ì¥í’ˆ ìœ í•´ì„± ì£¼ì˜ì‚¬í•­"
    try:
        web_results = search_prefer(query)
    except Exception as e:
        return f"(ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e})"

    prompt = f"""
    ì—­í• : ë‹¹ì‹ ì€ í™”ì¥í’ˆ ì•ˆì „ì„± ìš”ì•½ê°€ì…ë‹ˆë‹¤.
    ìë£Œ: ì•„ë˜ëŠ” ì„±ë¶„ ìœ„í—˜ì„± ê´€ë ¨ ì›¹ ê²€ìƒ‰ ìŠ¤ë‹ˆí«ì…ë‹ˆë‹¤.
    ---
    {web_results}
    ---
    ì…ë ¥ ì„±ë¶„(í›„ë³´): {', '.join(ingredients)}
    íš¨ëŠ¥ ì„±ë¶„(ê²¹ì¹¨ ì‹œ ê¸°ë³¸ ì œì™¸): {', '.join(efficacy_ings)}
    ì¼ë°˜ ì•ˆì „/ë³´ìŠµ ì„±ë¶„(íŠ¹ë³„ ê·¼ê±° ì—†ìœ¼ë©´ ì œì™¸): {', '.join(SAFE_BENIGN_INGS)}

    ì§€ì¹¨:
    1) 'í›„ë³´' ì¤‘ ì•„ë˜ ê¸°ì¤€ì— í•´ë‹¹í•˜ëŠ” ê²ƒë§Œ ê²½ê³  ëŒ€ìƒìœ¼ë¡œ ì±„íƒ:
    - ì•Œë ˆë¥´ê¸°/ìê·¹ ë³´ê³  ë¹ˆë„â†‘(í–¥ë£Œ/ì—ì„¼ì…œì˜¤ì¼, ë¦¬ëª¨ë„¨/ë¦¬ë‚ ë£°/ìœ ì œë†€ ë“±)
    - ì‚°/ë ˆí‹°ë…¸ì´ë“œ/ë²¤ì¡°ì¼í¼ì˜¥ì‚¬ì´ë“œ ë“± ê³ ë†ë„Â·pH ì˜ì¡´ ìê·¹ ê°€ëŠ¥
    - ë…¼ìŸì„± UV í•„í„°(ì˜¥ì‹œë²¤ì¡´/ì˜¥í‹°ë…¹ì„¸ì´íŠ¸ ë“±), í¬ë¦„ì•Œë°í•˜ì´ë“œ ë°©ì¶œ ë°©ë¶€ì œ ë“±
    2) 'íš¨ëŠ¥ ì„±ë¶„'ê³¼ ê²¹ì¹˜ë©´ ê¸°ë³¸ ì œì™¸. ë‹¨, ì¤‘ë“±ë„ ì´ìƒ ìœ„í—˜ ê·¼ê±°ê°€ ëª…í™•í•˜ë©´
    'ì¡°ê±´ë¶€'ë¡œ í‘œê¸°í•˜ê³  ì‚¬ìœ ë¥¼ 25ì ì´ë‚´ë¡œ.
    3) 'ì¼ë°˜ ì•ˆì „/ë³´ìŠµ' ë¦¬ìŠ¤íŠ¸ëŠ” íŠ¹ë³„í•œ ê·¼ê±°ê°€ ì—†ìœ¼ë©´ ì œì™¸.
    4) ì¤‘ë³µ ì œê±°, 3~5ê°œ ì´ë‚´, ì¤‘ìš”ë„ ìˆœ.
    ì¶œë ¥ í¬ë§·(ì´ í˜•ì‹ë§Œ):
    - ì„±ë¶„ â€” [ìœ„í—˜|ì¡°ê±´ë¶€] í•œì¤„ ì´ìœ 
    - ì„±ë¶„ â€” [ìœ„í—˜|ì¡°ê±´ë¶€] í•œì¤„ ì´ìœ 
    """
    resp = llm.invoke(prompt)
    txt = (resp.content or "").strip()
    if not txt:
        return ""
    # ê³¼ë„í•œ ê³µë°±/ë¹ˆ ì¤„ ì •ë¦¬ + 5ì¤„ ì´ë‚´ë¡œ ì œí•œ
    lines = [ln.strip() for ln in txt.splitlines() if ln.strip()]
    if len(lines) > 5:
        lines = lines[:5]
    return "\n".join(lines)


# ì œí’ˆë³„ 'ì¶”ì²œ ì´ìœ ' ì›¹ ìš”ì•½
def fetch_reasons_for_products(products: List[dict], selections: Dict[str, Any], key_ingredients: List[str]) -> List[str]:
    """ê° ì œí’ˆì— ëŒ€í•´ ì›¹ ìŠ¤ë‹ˆí«ì„ ë°”íƒ•ìœ¼ë¡œ 'ì™œ ì¶”ì²œí•˜ëŠ”ì§€' í•œ ì¤„ ìš”ì•½ì„ ë§Œë“ ë‹¤."""
    reasons: List[str] = []
    skin = selections.get("skin_type", "ì•Œ ìˆ˜ ì—†ìŒ")
    concerns = ", ".join([c for c in selections.get("concerns", []) if c and c != "ì•Œ ìˆ˜ ì—†ìŒ"]) or "ì•Œ ìˆ˜ ì—†ìŒ"
    category = selections.get("category", "ì•Œ ìˆ˜ ì—†ìŒ")

    import re
    for p in products[:3]:
        brand = (p.get("brand") or "").strip()
        name = (p.get("name") or "").strip()
        matched = ", ".join(sorted(set([m for m in p.get("found_ingredients", []) if m]))) or ", ".join(key_ingredients)

        # 1) ì›¹ ê²€ìƒ‰ (ë¶€ì¡±í•˜ë©´ ìë™ í´ë°±)
        query = f"{brand} {name} ì„±ë¶„ íš¨ê³¼ ë¦¬ë·° ì¥ë‹¨ì  {category} {skin} {concerns}"
        try:
            web_results = search_prefer(query)
        except Exception as e:
            web_results = f"(ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e})"

        # 2) ë§¤ìš° ì§§ì€ í•œ ì¤„ ìš”ì•½
        prompt = f"""
        ì—­í• : ë‹¹ì‹ ì€ í™”ì¥í’ˆ ì¶”ì²œ ê·¼ê±° ìš”ì•½ê°€ì…ë‹ˆë‹¤.
        ìƒí™©: ì‚¬ìš©ìëŠ” {skin} í”¼ë¶€, ê³ ë¯¼ì€ {concerns}, ì¹´í…Œê³ ë¦¬ëŠ” {category}ì…ë‹ˆë‹¤.
        ì œí’ˆ: {brand} {name}
        ë§¤ì¹­/í•µì‹¬ ì„±ë¶„: {matched}

        ìë£Œ(ì›¹ ê²€ìƒ‰ ìŠ¤ë‹ˆí«):
        ---
        {web_results}
        ---

        ê·œì¹™:
        - 'ì™œ ì´ ì œí’ˆì„ ì¶”ì²œí•˜ëŠ”ì§€' í•œ ì¤„(35~60ì)ë¡œ í•œêµ­ì–´ ìš”ì•½
        - ê°€ëŠ¥í•œ ê·¼ê±°: ë§¤ì¹­ ì„±ë¶„ íš¨ëŠ¥, ì„ìƒ/ë³´ìŠµ/ì§„ì • ì§€í‘œ, ì €ìê·¹(ë¬´í–¥/ì•½ì‚°ì„±), ë…¼ë€ ì„±ë¶„ ë¬´ì²¨ê°€ ë“±
        - ìë£Œì— ì—†ëŠ” ìˆ˜ì¹˜/ì‚¬ì‹¤ ì°½ì‘ ê¸ˆì§€
        - ìë£Œ ë¶€ì¡± ì‹œ ë§¤ì¹­ ì„±ë¶„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±
        - ì¶œë ¥: í•œ ì¤„ë§Œ, ë¶ˆë¦¿/ë¨¸ë¦¬ê¸°í˜¸/ë”°ì˜´í‘œ ì—†ì´, ë§ˆì¹¨í‘œ ì—†ì´
        """
        try:
            resp = llm.invoke(prompt)
            reason = (resp.content or "").strip().splitlines()[0]
        except Exception:
            reason = ""

        # ê³¼í•œ ë¨¸ë¦¬ê¸°í˜¸/ìˆ«ì ì œê±° + í´ë°±
        reason = re.sub(r"^[â€¢\-\*\d\.\)\s]+", "", reason) or "í•µì‹¬ ì„±ë¶„ê³¼ ì €ìê·¹ ì§€í‘œê°€ ì¡°ê±´ì— ë¶€í•©"
        reasons.append(reason)

    return reasons


# ===== LangGraph ë…¸ë“œ =====
def parse_user_input(state: Dict[str, Any]):
    """í˜„ì¬ ì…ë ¥ì„ íŒŒì‹±í•˜ê³ , ë¹„ëŠ” í•­ëª©ì€ 'ì´ì „ ëŒ€í™”ì˜ ìµœê·¼ ì¡°ê±´'ìœ¼ë¡œ ë°±í•„."""
    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€
    last = ""
    for m in reversed(state["messages"]):
        if isinstance(m, HumanMessage):
            last = _coerce_to_text(m.content)
            break

    # 1) ê·œì¹™ ê¸°ë°˜
    parsed = _rule_based_parse(last)

    # 2) 'ê°™ì€/ì´ì „ ì¡°ê±´' íŒíŠ¸ê°€ ìˆê±°ë‚˜ ë¹„ëŠ” í•­ëª©ì´ ìˆìœ¼ë©´, ê³¼ê±° ëŒ€í™”ë¡œ ë°±í•„
    if (
        any(kw in last for kw in ["ê°™ì€ ì¡°ê±´", "ì´ì „ ì¡°ê±´", "ë°©ê¸ˆì´ë‘", "ì•„ê¹Œë‘"])
        or parsed["skin_type"] == "ì•Œ ìˆ˜ ì—†ìŒ"
        or parsed["concerns"] == ["ì•Œ ìˆ˜ ì—†ìŒ"]
        or parsed["category"] == "ì•Œ ìˆ˜ ì—†ìŒ"
    ):
        defaults = _infer_prefs_from_history(state.get("messages", []))
        if parsed["skin_type"] == "ì•Œ ìˆ˜ ì—†ìŒ":
            parsed["skin_type"] = defaults.get("skin_type", "ì•Œ ìˆ˜ ì—†ìŒ")
        if parsed["concerns"] == ["ì•Œ ìˆ˜ ì—†ìŒ"]:
            parsed["concerns"] = defaults.get("concerns", ["ì•Œ ìˆ˜ ì—†ìŒ"])
        if parsed["category"] == "ì•Œ ìˆ˜ ì—†ìŒ":
            parsed["category"] = defaults.get("category", "ì•Œ ìˆ˜ ì—†ìŒ")

    # 3) ì—¬ì „íˆ ë¹„ë©´ LLM JSONìœ¼ë¡œ ë³´ì •(í˜„ì¬ ë¬¸ì¥ ê¸°ì¤€)
    if (
        parsed["skin_type"] == "ì•Œ ìˆ˜ ì—†ìŒ"
        or parsed["concerns"] == ["ì•Œ ìˆ˜ ì—†ìŒ"]
        or parsed["category"] == "ì•Œ ìˆ˜ ì—†ìŒ"
    ):
        fill = _llm_json_parse(last)
        if parsed["skin_type"] == "ì•Œ ìˆ˜ ì—†ìŒ":
            parsed["skin_type"] = fill["skin_type"]
        if parsed["concerns"] == ["ì•Œ ìˆ˜ ì—†ìŒ"]:
            parsed["concerns"] = fill["concerns"]
        if parsed["category"] == "ì•Œ ìˆ˜ ì—†ìŒ":
            parsed["category"] = fill["category"]

    # ìµœì¢… ì•ˆì „ì¥ì¹˜
    if not parsed.get("skin_type"):
        parsed["skin_type"] = "ì•Œ ìˆ˜ ì—†ìŒ"
    if not parsed.get("concerns"):
        parsed["concerns"] = ["ì•Œ ìˆ˜ ì—†ìŒ"]
    if not parsed.get("category"):
        parsed["category"] = "ì•Œ ìˆ˜ ì—†ìŒ"

    return {"user_selections": parsed}

def check_parsing_status(state: Dict[str, Any]):
    s = state["user_selections"]
    skin_ok = s.get("skin_type") and s["skin_type"] != "ì•Œ ìˆ˜ ì—†ìŒ"
    # âœ… í”¼ë¶€íƒ€ì…ë§Œ ìˆì–´ë„ ì§„í–‰
    return "success" if skin_ok else "clarification_needed"

def ask_for_clarification(state: Dict[str, Any]):
    s = state["user_selections"]
    missing = []
    if s.get("skin_type") == "ì•Œ ìˆ˜ ì—†ìŒ": missing.append("í”¼ë¶€ íƒ€ì…")
    if not s.get("concerns") or any(c == "ì•Œ ìˆ˜ ì—†ìŒ" for c in s.get("concerns", [])): missing.append("í”¼ë¶€ ê³ ë¯¼")
    if s.get("category") == "ì•Œ ìˆ˜ ì—†ìŒ": missing.append("ì œí’ˆ ì¢…ë¥˜")
    text = f"ë¶€ì¡±í•œ ì •ë³´({', '.join(missing)})ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. ì˜ˆ: 'ì§€ì„±, ë³´ìŠµ, ë¡œì…˜'"
    return {"messages": [AIMessage(content=text)]}

def get_ingredients(state: Dict[str, Any]):
    s = state["user_selections"]
    skin_type = s.get("skin_type", "ì•Œ ìˆ˜ ì—†ìŒ")
    concerns = s.get("concerns", ["ì•Œ ìˆ˜ ì—†ìŒ"])

    if concerns == ["ì•Œ ìˆ˜ ì—†ìŒ"]:
        prompt_text = f"""
        ì—­í• : í™”ì¥í’ˆ ì„±ë¶„ íë ˆì´í„°.
        ëª©í‘œ: '{skin_type}' í”¼ë¶€ì— ë³´í¸ì ìœ¼ë¡œ ì•ˆì „í•˜ê³  ìœ íš¨í•œ í•µì‹¬ í™œì„± ì„±ë¶„ 5ê°œë§Œ ì„ ì •.
        ì§€ì¹¨: ìê·¹ ë‚®ê³  ê·¼ê±° ê¸°ë°˜. ë³´ì¡°/ìš©ë§¤/ê°€í–¥/ë³´ì¡´ì œ ì œì™¸. UVí•„í„° ì œì™¸.
        ì¶œë ¥: ì‰¼í‘œë¡œë§Œ êµ¬ë¶„ëœ í•œ ì¤„
        """
    else:
        prompt_text = f"""
        ì—­í• : ë‹¹ì‹ ì€ í™”ì¥í’ˆ ì„±ë¶„ íë ˆì´í„°ì…ë‹ˆë‹¤.
        ëª©í‘œ: '{skin_type}' í”¼ë¶€ì˜ '{', '.join(concerns)}' ê³ ë¯¼ ê°œì„ ì— 'ì§ì ‘ì ìœ¼ë¡œ' ê¸°ì—¬í•˜ëŠ” í•µì‹¬ í™œì„± ì„±ë¶„ 5ê°œë§Œ ì„ ì •.
        ì§€ì¹¨:
        - ìê·¹ ìœ„í—˜ì´ ë‚®ê³  í•´ë‹¹ ê³ ë¯¼ì— ê·¼ê±° ê¸°ë°˜(ì„ìƒ/ë©”ì»¤ë‹ˆì¦˜)ì´ ìˆëŠ” 'í™œì„± ì„±ë¶„' ìœ„ì£¼.
        - ë‹¤ìŒ 'ì¼ë°˜ ë³´ì¡°/ì €ìœ„í—˜' ì„±ë¶„ì€ ì œì™¸: ê¸€ë¦¬ì„¸ë¦°, ë¶€í‹¸ë Œê¸€ë¼ì´ì½œ, í”„ë¡œíŒë‹¤ì´ì˜¬, 1,2-í—¥ì‚°ë‹¤ì´ì˜¬, ì •ì œìˆ˜, ì¹´ë³´ë¨¸ ë“±.
        - ìì™¸ì„ ì°¨ë‹¨ ì œí’ˆ ìƒí™©ì´ ì•„ë‹ˆë¼ë©´ UV í•„í„°ëŠ” ì œì™¸.
        - í–¥ë£Œ/ì—ì„¼ì…œì˜¤ì¼/ë³´ì¡´ì œ/ìš©ë§¤ ë“± ê¸°ëŠ¥ ë³´ì¡° ì„±ë¶„ì€ ì œì™¸.
        ì¶œë ¥: ì‰¼í‘œë¡œë§Œ êµ¬ë¶„ëœ í•œ ì¤„ (ì˜ˆ: íˆì•Œë£¨ë¡ ì‚°, ì„¸ë¼ë§ˆì´ë“œ, íŒí…Œë†€, ë³‘í’€ì¶”ì¶œë¬¼, ë‚˜ì´ì•„ì‹ ì•„ë§ˆì´ë“œ)
        """

    resp = llm.invoke(prompt_text)
    key_ingredients_str = resp.content.strip()
    return {"key_ingredients": [ing.strip().lower() for ing in key_ingredients_str.split(",") if ing.strip()]}


def find_products(state: Dict[str, Any]):
    selections = state["user_selections"]
    key_ingredients = state["key_ingredients"]
    cat = selections.get("category")

    if not cat or cat == "ì•Œ ìˆ˜ ì—†ìŒ":
        grouped = []
        for c in ALL_CATEGORIES:
            sel = {**selections, "category": c}
            top = find_and_rank_products("product_data.csv", sel, key_ingredients)[:2]  # ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ 2ê°œ
            if top:
                grouped.append({"category": c, "items": top})
        return {"top_products_by_cat": grouped}

    # ê¸°ì¡´ ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ ê²½ë¡œ
    top_products = find_and_rank_products("product_data.csv", selections, key_ingredients)
    return {"top_products": top_products}


def create_recommendation_message(state: Dict[str, Any]):
    """
    ê³ ì • í¬ë§·ìœ¼ë¡œ ìµœì¢… ë©”ì‹œì§€ë¥¼ ì¡°ë¦½í•©ë‹ˆë‹¤.
    - ìƒë‹¨: íš¨ëŠ¥ ì„±ë¶„(ë¶„ì„ ê¸°ì¤€)
    - ì œí’ˆ 1~3ìœ„: (ğŸ¥‡/ğŸ¥ˆ/ğŸ¥‰ + ë²ˆí˜¸) ì œí’ˆëª…
        - ë¸Œëœë“œ / ê°€ê²© / ìš©ëŸ‰ / ğŸ”— [ë§í¬]
        - ğŸ§ª íš¨ëŠ¥ ë§¤ì¹­ ì„±ë¶„
        - âœ… ì¶”ì²œ ì´ìœ (ì›¹ ìš”ì•½)
    - í•˜ë‹¨: âš ï¸ ì£¼ì˜í•´ì•¼ ë  ì„±ë¶„ (íš¨ëŠ¥ ì„±ë¶„ì€ ì œì™¸í•˜ì§€ ì•ŠìŒ â†’ LLMì´ [ì¡°ê±´ë¶€] íŒë‹¨)
    """
    import re
    from langchain_core.messages import AIMessage

    top = state.get("top_products", []) or []
    if not top:
        text = "ì¡°ê±´ì— ë§ëŠ” ì œí’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¡°ê±´ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ ë³´ì‹¤ë˜ìš”?"
        return {"messages": [AIMessage(content=text)], "recommendation_message": text}

    rank_emojis = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"]
    lines: List[str] = []

    # í—¤ë”
    lines.append("ìš”ì²­í•˜ì‹  ì¡°ê±´ì— ë§ì¶° ì¶”ì²œ ì œí’ˆì„ ì •ë¦¬í–ˆì–´ìš”. ğŸ˜Š")

    s = state.get("user_selections", {}) or {}
    skin = s.get("skin_type", "ì•Œ ìˆ˜ ì—†ìŒ")
    concerns_list = [c for c in s.get("concerns", []) if c and c != "ì•Œ ìˆ˜ ì—†ìŒ"]
    category = s.get("category", "ì•Œ ìˆ˜ ì—†ìŒ")

    # âœ… ì‚¬ìš©ì ì¡°ê±´: 'ì•Œ ìˆ˜ ì—†ìŒ'ì€ ê°ì¶”ê³ , ì (Â·) êµ¬ë¶„ìë¡œ ê¹”ë”í•˜ê²Œ
    cond_bits = []
    if skin != "ì•Œ ìˆ˜ ì—†ìŒ":
        cond_bits.append(f"ğŸ§‘â€ğŸ¦° í”¼ë¶€: **{skin}**")
    if concerns_list:
        cond_bits.append(f"ğŸŒ¿ ê³ ë¯¼: **{', '.join(concerns_list)}**")
    if category != "ì•Œ ìˆ˜ ì—†ìŒ":
        cond_bits.append(f"ğŸ§´ ì œí’ˆ: **{category}**")

    if cond_bits:
        lines.append("**ğŸ¯ ì‚¬ìš©ì ì¡°ê±´**")
        lines.append("   " + "   Â·   ".join(cond_bits))
        lines.append("")

    # 1) ìƒë‹¨: íš¨ëŠ¥ ì„±ë¶„(ë¶„ì„ ê¸°ì¤€)
    key_ings = [k.strip() for k in state.get("key_ingredients", []) if k and str(k).strip()]
    if key_ings:
        lines.append(f"**ğŸ§ª íš¨ëŠ¥ ì„±ë¶„(ë¶„ì„ ê¸°ì¤€):** {', '.join(key_ings)}")
        lines.append("")  # í•œ ì¤„ ì—¬ë°±

    # âœ… ì œí’ˆë³„ 'ì¶”ì²œ ì´ìœ ' ì›¹ ìš”ì•½
    selections = state.get("user_selections", {})
    web_reasons = fetch_reasons_for_products(top, selections, key_ings)

    # 2) ì œí’ˆ ì¹´ë“œ (ìƒìœ„ 3ê°œ)
    for i, p in enumerate(top[:3]):
        emoji = rank_emojis[i] if i < len(rank_emojis) else f"{i+1}."
        name = (p.get("name") or "").strip()
        brand = (p.get("brand") or "").strip()
        price = p.get("price", "?")
        volume = p.get("volume", "?")
        link = (p.get("link") or "").strip()
        link_md = f"[ë§í¬]({link})" if link else "-"

        matched_raw = [m.strip() for m in p.get("found_ingredients", []) if m and str(m).strip()]
        matched_unique = ", ".join(sorted(set(matched_raw))) if matched_raw else "ì—†ìŒ (ê¸°ì¤€ ì„±ë¶„ê³¼ ì§ì ‘ ë§¤ì¹­ ì—†ìŒ)"

        lines.append(f"{emoji} {i+1}. {name}")
        lines.append(f"   ğŸ¬ ë¸Œëœë“œ: {brand or '-'}")
        lines.append(f"   ğŸ’° ê°€ê²©: {price}")
        lines.append(f"   ğŸ«™ ìš©ëŸ‰: {volume}")
        lines.append(f"   ğŸ”— {link_md}")
        lines.append(f"   ğŸ§ª íš¨ëŠ¥ ë§¤ì¹­ ì„±ë¶„: {matched_unique}")
        reason = web_reasons[i] if i < len(web_reasons) else "í•µì‹¬ ì„±ë¶„ê³¼ ì €ìê·¹ ì§€í‘œê°€ ì¡°ê±´ì— ë¶€í•©"
        lines.append(f"   âœ… ì¶”ì²œ ì´ìœ : {reason}")
        if i < 2:
            lines.append("")

    # 3) ì£¼ì˜ ì„±ë¶„ (íš¨ëŠ¥ ì„±ë¶„ì€ ì œì™¸í•˜ì§€ ì•Šê³  LLMì´ [ì¡°ê±´ë¶€]/[ìœ„í—˜] íŒë‹¨)
    all_found = [str(ing).strip().lower() for p in top for ing in p.get("found_ingredients", []) if str(ing).strip()]
    unique_found = sorted(set(all_found))

    key_set = {k.lower() for k in key_ings}
    safe_set = {s.lower() for s in SAFE_BENIGN_INGS}

    # âœ… íš¨ëŠ¥ ì„±ë¶„ì€ í•„í„°ë§í•˜ì§€ ì•ŠìŒ(ê²¹ì¹˜ë©´ í”„ë¡¬í”„íŠ¸ê°€ [ì¡°ê±´ë¶€]ë¡œ í‘œê¸°)
    #    ì•ˆì „/ë³´ì¡° ì„±ë¶„ë§Œ ì„ ì œì ìœ¼ë¡œ ì œì™¸í•˜ì—¬ ë…¸ì´ì¦ˆ ì¶•ì†Œ
    warn_candidates = [ing for ing in unique_found if ing not in safe_set]

    warnings_raw = fetch_warnings_for_ingredients(
        warn_candidates,
        efficacy_ings=sorted(list(key_set))
    ) if warn_candidates else ""

    # âœ… í´ë°±: íš¨ëŠ¥ ì„±ë¶„ê³¼ ê²¹ì¹˜ëŠ” í›„ë³´ê°€ ìˆëŠ”ë°ë„ ìš”ì•½ì´ ë¹„ë©´, ìµœì†Œí•œ [ì¡°ê±´ë¶€] í•œ ì¤„ ìƒì„±
    overlap = [ing for ing in warn_candidates if ing in key_set]
    if (not warnings_raw) and overlap:
        fallback_lines = [
            f"{ing} â€” [ì¡°ê±´ë¶€] í•µì‹¬ ì„±ë¶„ê³¼ ê²¹ì¹¨, ê°œì¸ì°¨ì— ë”°ë¼ ìê·¹ ê°€ëŠ¥"
            for ing in overlap[:3]
        ]
        warnings_raw = "\n".join(fallback_lines)

    lines.append("")
    lines.append("âš ï¸ ì£¼ì˜í•´ì•¼ ë  ì„±ë¶„:")
    if warnings_raw:
        w_lines = [ln.strip() for ln in warnings_raw.splitlines() if ln.strip()]
        for ln in w_lines:
            clean = re.sub(r"^[â€¢\-\*\d\.\)\s]+", "", ln)
            lines.append(f"âš ï¸ {clean}")
    else:
        lines.append("â€¢ íŠ¹ë³„íˆ ì£¼ì˜í•´ì•¼ ë  ì„±ë¶„ì€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    final_text = "\n".join(lines)
    return {"messages": [AIMessage(content=final_text)], "recommendation_message": final_text}

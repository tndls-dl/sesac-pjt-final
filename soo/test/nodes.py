import re
import json
from typing import Dict, Any, List

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage

from utils import find_and_rank_products
from langchain_community.tools.tavily_search import TavilySearchResults

# ===== ëª¨ë¸ & ì›¹ê²€ìƒ‰ =====
llm = ChatOpenAI(model_name="gpt-4o", temperature=0)
search = TavilySearchResults(k=3)

# ===== ë™ì˜ì–´/ì •ê·œí™” =====
SKIN_TYPES = {"ì§€ì„±", "ê±´ì„±", "ë³µí•©ì„±", "ë¯¼ê°ì„±", "ì•„í† í”¼ì„±"}
CONCERNS = {"ë³´ìŠµ", "ì§„ì •", "ë¯¸ë°±", "ì£¼ë¦„/íƒ„ë ¥", "ëª¨ê³µì¼€ì–´", "í”¼ì§€ì¡°ì ˆ", "ì£¼ë¦„", "íƒ„ë ¥"}
CATEGORY_SYNONYMS = {
    "í† ë„ˆ": "ìŠ¤í‚¨/í† ë„ˆ", "ìŠ¤í‚¨": "ìŠ¤í‚¨/í† ë„ˆ", "ìŠ¤í‚¨/í† ë„ˆ": "ìŠ¤í‚¨/í† ë„ˆ",
    "ë¡œì…˜": "ë¡œì…˜/ì—ë©€ì ¼", "ì—ë©€ì „": "ë¡œì…˜/ì—ë©€ì ¼", "ì—ë©€ì ¼": "ë¡œì…˜/ì—ë©€ì ¼", "ë¡œì…˜/ì—ë©€ì ¼": "ë¡œì…˜/ì—ë©€ì ¼",
    "ì„¸ëŸ¼": "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼", "ì•°í”Œ": "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼", "ì—ì„¼ìŠ¤": "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼", "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼": "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼",
    "í¬ë¦¼": "í¬ë¦¼",
    "ë°¤": "ë°¤/ë©€í‹°ë°¤", "ë©€í‹°ë°¤": "ë°¤/ë©€í‹°ë°¤", "ë°¤/ë©€í‹°ë°¤": "ë°¤/ë©€í‹°ë°¤",
    "í´ë Œì§•í¼": "í´ë Œì§• í¼", "í´ë Œì§•": "í´ë Œì§• í¼", "í´ë Œì§• í¼": "í´ë Œì§• í¼",
    "ì‹œíŠ¸ë§ˆìŠ¤í¬": "ì‹œíŠ¸ë§ˆìŠ¤í¬",
    "ì„ í¬ë¦¼": "ì„ í¬ë¦¼/ë¡œì…˜", "ì„ ë¡œì…˜": "ì„ í¬ë¦¼/ë¡œì…˜", "ìì™¸ì„ ì°¨ë‹¨ì œ": "ì„ í¬ë¦¼/ë¡œì…˜", "ì„ í¬ë¦¼/ë¡œì…˜": "ì„ í¬ë¦¼/ë¡œì…˜",
}

# ===== ê³µí†µ ìœ í‹¸ =====
def _coerce_to_text(x) -> str:
    if isinstance(x, str):
        return x
    if isinstance(x, list):
        parts = []
        for item in x:
            if isinstance(item, dict) and item.get("type") == "text":
                parts.append(item.get("text", ""))
            elif isinstance(item, str):
                parts.append(item)
        return " ".join(p for p in parts if p).strip()
    return str(x)

def _normalize_tokens(text: str) -> List[str]:
    if not isinstance(text, str):
        text = _coerce_to_text(text)
    cleaned = re.sub(r"[\"'`]+", "", text)
    raw = re.split(r"[,/]\s*|\s+", cleaned)
    return [t.strip() for t in raw if t.strip()]

def _messages_to_text(messages, limit=30) -> str:
    """ìµœê·¼ ë©”ì‹œì§€ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ë³‘í•©(ì»¨í…ìŠ¤íŠ¸ ì¶”ë¡ ìš©)."""
    out = []
    for m in messages[-limit:]:
        role = "ì‚¬ìš©ì" if isinstance(m, HumanMessage) else "ë„ìš°ë¯¸"
        out.append(f"{role}: {_coerce_to_text(m.content)}")
    return "\n".join(out)

# ===== íŒŒì‹± ë¡œì§ =====
def _rule_based_parse(s: str) -> Dict[str, Any]:
    tokens = _normalize_tokens(s)
    skin = None
    concerns: List[str] = []
    category = None
    for t in tokens:
        if t in SKIN_TYPES:
            skin = t; continue
        if t in CONCERNS:
            concerns.append("ì£¼ë¦„/íƒ„ë ¥" if t in {"ì£¼ë¦„", "íƒ„ë ¥"} else t); continue
        if t in CATEGORY_SYNONYMS:
            category = CATEGORY_SYNONYMS[t]; continue
        if t.endswith("í† ë„ˆ"):
            category = "ìŠ¤í‚¨/í† ë„ˆ"
        elif t in {"ì„¸ëŸ¼", "ì•°í”Œ", "ì—ì„¼ìŠ¤"}:
            category = "ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼"

    return {
        "skin_type": skin or "ì•Œ ìˆ˜ ì—†ìŒ",
        "concerns": concerns or ["ì•Œ ìˆ˜ ì—†ìŒ"],
        "category": category or "ì•Œ ìˆ˜ ì—†ìŒ",
    }

def _llm_json_parse(s: str) -> Dict[str, Any]:
    prompt = f"""
ì•„ë˜ ë¬¸ì¥ì—ì„œ ì‚¬ìš©ì ì •ë³´ë¥¼ JSONìœ¼ë¡œë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
- í”¼ë¶€ íƒ€ì…: ë¯¼ê°ì„±, ì§€ì„±, ê±´ì„±, ì•„í† í”¼ì„±, ë³µí•©ì„±
- í”¼ë¶€ ê³ ë¯¼: ë³´ìŠµ, ì§„ì •, ë¯¸ë°±, ì£¼ë¦„/íƒ„ë ¥, ëª¨ê³µì¼€ì–´, í”¼ì§€ì¡°ì ˆ
- ì œí’ˆ ì¢…ë¥˜: ìŠ¤í‚¨/í† ë„ˆ, ë¡œì…˜/ì—ë©€ì ¼, ì—ì„¼ìŠ¤/ì•°í”Œ/ì„¸ëŸ¼, í¬ë¦¼, ë°¤/ë©€í‹°ë°¤, í´ë Œì§• í¼, ì‹œíŠ¸ë§ˆìŠ¤í¬, ì„ í¬ë¦¼/ë¡œì…˜
ëª» ì°¾ìœ¼ë©´ "ì•Œ ìˆ˜ ì—†ìŒ"ìœ¼ë¡œ ì±„ì›Œì£¼ì„¸ìš”.

ë°˜ë“œì‹œ ìˆœìˆ˜ JSONë§Œ:
{{
  "skin_type": "...",
  "concerns": ["..."],
  "category": "..."
}}

ì…ë ¥: {s}
"""
    resp = llm.invoke(prompt).content.strip()
    try:
        if "{" in resp and "}" in resp:
            resp = resp[resp.index("{"): resp.rindex("}") + 1]
        data = json.loads(resp)
    except Exception:
        data = {"skin_type": "ì•Œ ìˆ˜ ì—†ìŒ", "concerns": ["ì•Œ ìˆ˜ ì—†ìŒ"], "category": "ì•Œ ìˆ˜ ì—†ìŒ"}

    if isinstance(data.get("concerns"), str):
        data["concerns"] = [data["concerns"]]
    cat = (data.get("category") or "").strip()
    data["category"] = CATEGORY_SYNONYMS.get(cat, cat if cat else "ì•Œ ìˆ˜ ì—†ìŒ")
    return data

def _infer_prefs_from_history(messages) -> Dict[str, Any]:
    """ì´ì „ ëŒ€í™”ì—ì„œ ê°€ì¥ ìµœê·¼ì— í™•ì •ëœ ì¡°ê±´ì„ JSONìœ¼ë¡œ ì¶”ì¶œ."""
    history_text = _messages_to_text(messages, limit=30)
    prompt = f"""
ì•„ë˜ ëŒ€í™” ê¸°ë¡ì—ì„œ ê°€ì¥ ìµœê·¼ì— í™•ì •ëœ ì‚¬ìš©ì ì¡°ê±´ì„ JSONìœ¼ë¡œë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
ëª» ì°¾ìœ¼ë©´ "ì•Œ ìˆ˜ ì—†ìŒ"ìœ¼ë¡œ ì±„ì›Œì£¼ì„¸ìš”.

ë°˜ë“œì‹œ ìˆœìˆ˜ JSONë§Œ:
{{
  "skin_type": "...",
  "concerns": ["..."],
  "category": "..."
}}

ëŒ€í™” ê¸°ë¡:
{history_text}
"""
    resp = llm.invoke(prompt).content.strip()
    try:
        if "{" in resp and "}" in resp:
            resp = resp[resp.index("{"): resp.rindex("}") + 1]
        data = json.loads(resp)
    except Exception:
        data = {"skin_type": "ì•Œ ìˆ˜ ì—†ìŒ", "concerns": ["ì•Œ ìˆ˜ ì—†ìŒ"], "category": "ì•Œ ìˆ˜ ì—†ìŒ"}

    if isinstance(data.get("concerns"), str):
        data["concerns"] = [data["concerns"]]
    cat = (data.get("category") or "").strip()
    data["category"] = CATEGORY_SYNONYMS.get(cat, cat if cat else "ì•Œ ìˆ˜ ì—†ìŒ")
    return data

# ===== ì›¹ ê²€ìƒ‰(ì£¼ì˜ ì„±ë¶„) =====
def fetch_warnings_for_ingredients(ingredients: List[str]) -> str:
    """ìœ í•´ ê°€ëŠ¥ ì„±ë¶„ë“¤ì— ëŒ€í•´ ì›¹ ê²€ìƒ‰ í›„ ìš”ì•½. (ì¶”ê°€ ì œí’ˆ ì¶”ì²œ X)"""
    if not ingredients:
        return ""
    query = f"{', '.join(ingredients)} í™”ì¥í’ˆ ìœ í•´ì„± ì£¼ì˜ì‚¬í•­"
    try:
        web_results = search.run(query)
    except Exception as e:
        return f"(ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e})"
    prompt = f"""
ë‹¤ìŒì€ í™”ì¥í’ˆ ì„±ë¶„ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤:
{web_results}

ê° ì„±ë¶„ì´ ì™œ ì£¼ì˜í•´ì•¼ í•˜ëŠ”ì§€, 'ì„±ë¶„ëª… - í•œ ì¤„ ìš”ì•½' í˜•íƒœë¡œ ê°„ë‹¨íˆ ì •ë¦¬í•´ ì£¼ì„¸ìš”.
ì¶”ê°€ ì œí’ˆ ì¶”ì²œì€ ê¸ˆì§€í•©ë‹ˆë‹¤.
"""
    resp = llm.invoke(prompt)
    return resp.content.strip()

# ===== LangGraph ë…¸ë“œ =====
def parse_user_input(state: Dict[str, Any]):
    """í˜„ì¬ ì…ë ¥ì„ íŒŒì‹±í•˜ê³ , ë¹„ëŠ” í•­ëª©ì€ 'ì´ì „ ëŒ€í™”ì˜ ìµœê·¼ ì¡°ê±´'ìœ¼ë¡œ ë°±í•„."""
    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€
    last = ""
    for m in reversed(state["messages"]):
        if isinstance(m, HumanMessage):
            last = _coerce_to_text(m.content)
            break

    # 1) ê·œì¹™ ê¸°ë°˜
    parsed = _rule_based_parse(last)

    # 2) 'ê°™ì€/ì´ì „ ì¡°ê±´' íŒíŠ¸ê°€ ìˆê±°ë‚˜ ë¹„ëŠ” í•­ëª©ì´ ìˆìœ¼ë©´, ê³¼ê±° ëŒ€í™”ë¡œ ë°±í•„
    if (
        any(kw in last for kw in ["ê°™ì€ ì¡°ê±´", "ì´ì „ ì¡°ê±´", "ë°©ê¸ˆì´ë‘", "ì•„ê¹Œë‘"])
        or parsed["skin_type"] == "ì•Œ ìˆ˜ ì—†ìŒ"
        or parsed["concerns"] == ["ì•Œ ìˆ˜ ì—†ìŒ"]
        or parsed["category"] == "ì•Œ ìˆ˜ ì—†ìŒ"
    ):
        defaults = _infer_prefs_from_history(state.get("messages", []))
        if parsed["skin_type"] == "ì•Œ ìˆ˜ ì—†ìŒ":
            parsed["skin_type"] = defaults.get("skin_type", "ì•Œ ìˆ˜ ì—†ìŒ")
        if parsed["concerns"] == ["ì•Œ ìˆ˜ ì—†ìŒ"]:
            parsed["concerns"] = defaults.get("concerns", ["ì•Œ ìˆ˜ ì—†ìŒ"])
        if parsed["category"] == "ì•Œ ìˆ˜ ì—†ìŒ":
            parsed["category"] = defaults.get("category", "ì•Œ ìˆ˜ ì—†ìŒ")

    # 3) ì—¬ì „íˆ ë¹„ë©´ LLM JSONìœ¼ë¡œ ë³´ì •(í˜„ì¬ ë¬¸ì¥ ê¸°ì¤€)
    if (
        parsed["skin_type"] == "ì•Œ ìˆ˜ ì—†ìŒ"
        or parsed["concerns"] == ["ì•Œ ìˆ˜ ì—†ìŒ"]
        or parsed["category"] == "ì•Œ ìˆ˜ ì—†ìŒ"
    ):
        fill = _llm_json_parse(last)
        if parsed["skin_type"] == "ì•Œ ìˆ˜ ì—†ìŒ":
            parsed["skin_type"] = fill["skin_type"]
        if parsed["concerns"] == ["ì•Œ ìˆ˜ ì—†ìŒ"]:
            parsed["concerns"] = fill["concerns"]
        if parsed["category"] == "ì•Œ ìˆ˜ ì—†ìŒ":
            parsed["category"] = fill["category"]

    # ìµœì¢… ì•ˆì „ì¥ì¹˜
    if not parsed.get("skin_type"):
        parsed["skin_type"] = "ì•Œ ìˆ˜ ì—†ìŒ"
    if not parsed.get("concerns"):
        parsed["concerns"] = ["ì•Œ ìˆ˜ ì—†ìŒ"]
    if not parsed.get("category"):
        parsed["category"] = "ì•Œ ìˆ˜ ì—†ìŒ"

    return {"user_selections": parsed}

def check_parsing_status(state: Dict[str, Any]):
    s = state["user_selections"]
    skin_ok = s.get("skin_type") and s["skin_type"] != "ì•Œ ìˆ˜ ì—†ìŒ"
    con_ok = s.get("concerns") and any(c != "ì•Œ ìˆ˜ ì—†ìŒ" for c in s["concerns"])
    cat_ok = s.get("category") and s["category"] != "ì•Œ ìˆ˜ ì—†ìŒ"
    return "success" if (skin_ok and con_ok and cat_ok) else "clarification_needed"

def ask_for_clarification(state: Dict[str, Any]):
    s = state["user_selections"]
    missing = []
    if s.get("skin_type") == "ì•Œ ìˆ˜ ì—†ìŒ": missing.append("í”¼ë¶€ íƒ€ì…")
    if not s.get("concerns") or any(c == "ì•Œ ìˆ˜ ì—†ìŒ" for c in s.get("concerns", [])): missing.append("í”¼ë¶€ ê³ ë¯¼")
    if s.get("category") == "ì•Œ ìˆ˜ ì—†ìŒ": missing.append("ì œí’ˆ ì¢…ë¥˜")
    text = f"ë¶€ì¡±í•œ ì •ë³´({', '.join(missing)})ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. ì˜ˆ: 'ì§€ì„±, ë³´ìŠµ, ë¡œì…˜'"
    return {"messages": [AIMessage(content=text)]}

def get_ingredients(state: Dict[str, Any]):
    s = state["user_selections"]
    skin_type = s.get("skin_type", "ì•Œ ìˆ˜ ì—†ìŒ")
    concerns = s.get("concerns", ["ì•Œ ìˆ˜ ì—†ìŒ"])
    prompt_text = f"""
'{skin_type}' í”¼ë¶€ì˜ '{', '.join(concerns)}' ê³ ë¯¼ì— ì¢‹ì€ í•µì‹¬ ì„±ë¶„ 5ê°€ì§€ë¥¼ ë½‘ì•„ ì£¼ì„¸ìš”.
ì‰¼í‘œ(,)ë¡œë§Œ êµ¬ë¶„ëœ ëª©ë¡ìœ¼ë¡œ ê°„ë‹¨íˆ ë‹µë³€ (ì˜ˆ: íˆì•Œë£¨ë¡ ì‚°,ì„¸ë¼ë§ˆì´ë“œ,íŒí…Œë†€,ë³‘í’€ì¶”ì¶œë¬¼,ìŠ¤ì¿ ì•Œë€)
"""
    resp = llm.invoke(prompt_text)
    key_ingredients_str = resp.content.strip()
    return {"key_ingredients": [ing.strip().lower() for ing in key_ingredients_str.split(",") if ing.strip()]}

def find_products(state: Dict[str, Any]):
    selections = state["user_selections"]
    key_ingredients = state["key_ingredients"]
    top_products = find_and_rank_products("product_data.csv", selections, key_ingredients)
    return {"top_products": top_products}

def create_recommendation_message(state: Dict[str, Any]):
    """
    ê³ ì • í¬ë§·ìœ¼ë¡œ ìµœì¢… ë©”ì‹œì§€ë¥¼ ì¡°ë¦½í•©ë‹ˆë‹¤.
    - ìƒë‹¨: íš¨ëŠ¥ ì„±ë¶„(ë§¤ì¹­ ê¸°ì¤€) ëª©ë¡
    - ì œí’ˆ ë¦¬ìŠ¤íŠ¸: (ì´ëª¨ì§€ + ë²ˆí˜¸) ì œí’ˆëª… / ê°€ê²© / ìš©ëŸ‰ / [ë§í¬ ì—´ê¸°]
                  + ê° ì œí’ˆë³„ "ë§¤ì¹­ ì„±ë¶„" í‘œì‹œ
    - ì£¼ì˜ ì„±ë¶„: fetch_warnings_for_ingredients(...) ìš”ì•½(ì›¹ê²€ìƒ‰)
    """
    top = state.get("top_products", [])
    if not top:
        text = "ì¡°ê±´ì— ë§ëŠ” ì œí’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¡°ê±´ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ ë³´ì‹¤ë˜ìš”?"
        return {"messages": [AIMessage(content=text)], "recommendation_message": text}

    rank_emojis = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"]
    lines: List[str] = []

    # 1) ìƒë‹¨: íš¨ëŠ¥ ì„±ë¶„(ë§¤ì¹­ ê¸°ì¤€)
    key_ings = [k.strip() for k in state.get("key_ingredients", []) if k and k.strip()]
    if key_ings:
        lines.append("ğŸ§ª íš¨ëŠ¥ ì„±ë¶„(ë§¤ì¹­ ê¸°ì¤€): " + ", ".join(key_ings))
        lines.append("")  # í•œ ì¤„ ì—¬ë°±

    # 2) ì œí’ˆ 1~3ìœ„ ì¹´ë“œ
    for i, p in enumerate(top[:3]):
        emoji = rank_emojis[i] if i < len(rank_emojis) else f"{i+1}."
        brand = (p.get("brand") or "").strip()
        name = (p.get("name") or "").strip()
        price = p.get("price", "?")
        volume = p.get("volume", "?")
        link = (p.get("link") or "").strip()
        link_md = f"[ì—´ê¸°]({link})" if link else "-"

        # ë§¤ì¹­ ì„±ë¶„ (í•´ë‹¹ ì œí’ˆ ì „ì„±ë¶„ì—ì„œ ì‹¤ì œë¡œ ì¡íŒ ê²ƒë“¤)
        matched_raw = [m.strip() for m in p.get("found_ingredients", []) if m and str(m).strip()]
        matched_unique = ", ".join(sorted(set(matched_raw))) if matched_raw else "ì—†ìŒ (ê¸°ì¤€ ì„±ë¶„ê³¼ ì§ì ‘ ë§¤ì¹­ ì—†ìŒ)"

        lines.append(f"{emoji} {i+1}. {brand} {name}")
        lines.append(f"   ğŸ’° ê°€ê²©: {price}")
        lines.append(f"   ğŸ«™ ìš©ëŸ‰: {volume}")
        lines.append(f"   ğŸ”— ë§í¬: {link_md}")
        lines.append(f"   ğŸ§ª íš¨ëŠ¥ ì„±ë¶„: {matched_unique}")
        if i < 2:  # ë§ˆì§€ë§‰ ì•„ì´í…œ ë’¤ì—ëŠ” ë¶ˆí•„ìš”í•œ ê³µë°± ë°©ì§€
            lines.append("")

    # 3) ì£¼ì˜ ì„±ë¶„ (ì›¹ ê²€ìƒ‰ ìš”ì•½) â€” ë¶ˆí•„ìš”í•œ ê³µë°± ì¤„ ì œê±° ì²˜ë¦¬ í¬í•¨
    all_found = [ing for p in top for ing in p.get("found_ingredients", [])]
    unique_found = sorted({ing for ing in all_found if ing})
    warnings_raw = fetch_warnings_for_ingredients(unique_found) if unique_found else ""

    lines.append("")
    lines.append("âš ï¸ ì£¼ì˜í•´ì•¼ ë  ì„±ë¶„:")
    if warnings_raw:
        import re
        # ë¹ˆ ì¤„ ì œê±° + ê¸€ë¨¸ë¦¬ ê¸°í˜¸ í†µì¼
        w_lines = [ln.strip() for ln in warnings_raw.splitlines() if ln.strip()]
        if len(w_lines) == 1:
            clean = re.sub(r"^[â€¢\-\*\d\.\)\s]+", "", w_lines[0])
            lines.append(f"â€¢ {clean}")
        else:
            for ln in w_lines:
                clean = re.sub(r"^[â€¢\-\*\d\.\)\s]+", "", ln)
                lines.append(f"â€¢ {clean}")
    else:
        lines.append("â€¢ íŠ¹ë³„íˆ ì£¼ì˜ ì„±ë¶„ì€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    final_text = "\n".join(lines)
    return {"messages": [AIMessage(content=final_text)], "recommendation_message": final_text}